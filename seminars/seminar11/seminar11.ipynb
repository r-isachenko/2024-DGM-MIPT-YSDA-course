{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-humidity",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Generative Models</center>\n",
    "## <center>Seminar 11</center>\n",
    "\n",
    "<center><img src=\"pics/AIMastersLogo.png\" width=600 /></center>\n",
    "<center>08.05.2024</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9313cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plan\n",
    "\n",
    "1. Noise Conditioned Scored Network\n",
    "2. Gaussian Difussion process\n",
    "3. Denoising diffusion propbabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396708f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Noise Conditioned Scored Network\n",
    "Что помните?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655dbdb1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Зашумляем с использованием разных $\\sigma_1 < \\sigma_2 ... < \\sigma_T$\n",
    "2. Учим модель для разных $\\sigma$. Модель знает, для какого $t$ она предсказывает шум\n",
    "3. Семплируем при помощи *Langevin dynamic* - итеративный процесс, когда по чуть-чуть \"скатываемся\" в $\\pi(x)$\n",
    "4. В конце процесса зашумления получаем $N(0, \\sigma_T \\cdot I)$ - чтобы можно было семплировать"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440dc335",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508023849.png\" width=700 /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6b3cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508023849.png\" width=900 /></center>\n",
    "\n",
    "\n",
    "**Вопрос:** Если мы умеем хорошо предсказывать шум / \"расшумлять\", то почему бы сразу не расшумлять?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def7db3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508023849.png\" width=900 /></center>\n",
    "\n",
    "\n",
    "**Вопрос:** Как учить?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d31ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Как учить?**\n",
    "<center><img src=\"attachments/Pasted image 20240430191355.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99049f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Вопрос:** Как семплировать?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a60b11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Как семплировать?**\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240508023930.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6f480",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Вопрос:** Какой критерий остановки?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0437780",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Примечание**\n",
    "\n",
    "Чтобы обусловиться на $\\sigma_t$ у score-модели мы учим $s(x_t, t)$. Например: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96005199",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class ConditionedResnetBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_embeddings: int) -> None:\n",
    "        super().__init__()\n",
    "        # you could experiment with this architecture\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dim, dim, kernel_size=1),\n",
    "        )\n",
    "        self.dim = dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        time_embed = self.embedding(y).view(-1, self.dim, 1, 1)\n",
    "        return x + self.block(x + time_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a609ae9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Вспомним теорию\n",
    "**Как пришли к этой модели?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f247e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Langevin dynamic / Score matching** - не нужно знать $\\pi(x)$, нужно знать $\\nabla_x \\pi(x)$ и можно итеративно семплировать\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240429131855.png\" width=800 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56a2bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240429131855.png\" width=800 /></center>\n",
    "\n",
    "**Вопрос:** Не знаем $\\pi(x)$, умеем семплировать. Что делать? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb302c1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. **Denosing score matching** - зашумим предсказания и будем предсказывать шум, НО $\\nabla_{x_{\\sigma}}q(x_{\\sigma}|x)$ можно будет посчитать!\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240508024107.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c56667",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Вопрос:** В чем обман?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df8be1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508024107.png\" width=1000 /></center>\n",
    "\n",
    "1. Ключевая идея теоремы - разложить матожидание для зашумленных семплов $\\mathbf{E}_{q(x_{\\sigma})} = \\mathbf{E}_{\\pi(x)}\\mathbf{E}_{q(x_{\\sigma}|x)}$\n",
    "2. **В чем обман?** \"локальная\" score функция"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd7c08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Итого\n",
    "\n",
    "1. **Langevin dynamic / Score matching** - не знаем $\\pi(x)$, но знаем $\\nabla_x \\pi(x)$ $\\Rightarrow$ можно семплировать из $\\pi(x)$\n",
    "2. **Denosing score matching** - обучаем модель денойзить, получим \"локальные\" score функции \n",
    "3. **Noice Conditioned Score Network** - учим $s(t, x_t)$ для набора $\\sigma_1 < \\sigma_2 ... < \\sigma_T$\n",
    "\t1. $q(x_1) \\sim \\pi(x), q(x_T) \\sim N(0, \\sigma_T \\cdot I)$\n",
    "\t2. Семплируем при помощи *Langevin dynamic*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b0f27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Diffusion process\n",
    "\n",
    "Что помните?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7503ef9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Forward gaussian diffusion process** - по чуть-чуть \"впрыскивают\" шум, картинка превращается в $N(0, I)$\n",
    "2. **Reverse gaussian diffusion process** - восстанавливаем картинку из $N(0, I)$, учим модель предсказывать шум и по чуть-чуть денойзим\n",
    "3. На практике - учим модель денойзить зашумленную картинку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c90ce7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Вспоминаем математику"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c25b40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Идея - можем постепенно зашумить картинку так, чтобы прийти в $N(0, I)$\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240508024010.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0cbc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508024215.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab95b23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508024215.png\" width=1000 /></center>\n",
    "\n",
    "**Примечания:**\n",
    "1. $q(x_{\\infty}|x_{0}) = N(0, I)$ - домнажая на число 0 < $\\sqrt{1 - \\beta}$ < 1 размываем исходную картинку\n",
    "2. $p_{\\infty}(x) = N(0, I)$ - мат. ожидание размыли в 0, намешали кучу нормального шума - получили нормальный шум\n",
    "3. Марковость $q(x_{t}|x_{t-1})$ - нужно будет в выводе того, как делать обратный процесс\n",
    "4. Процесс определяется $\\beta$-policy и $T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57923730",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508024215.png\" width=1000 /></center>\n",
    "\n",
    "**Вопрос:** Можем ли мы явно обратить процесс?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6ecc9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. $q(x_{t-1}|x_{t}) = \\dfrac{q(x_t|x_{t-1})q(x_{t-1})}{q(x_t)}$\n",
    "2. $q(x_t) = \\int p(x_t|x_0) \\pi(x_0) dx_0$ - мы такое считать не умеем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9c38f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508024215.png\" width=1000 /></center>\n",
    "\n",
    "**Вопрос:** Можем ли сразу зашумлять данные так, чтобы все явно обращалось? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1db43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508024215.png\" width=1000 /></center>\n",
    "\n",
    "**Вопрос:** Что делаем, когда не знаем как посчитать $q(x_{t-1}|x_t)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c705206",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Учим модель $p_{\\theta, t}(x_{t-1}|x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003421c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240508024215.png\" width=1000 /></center>\n",
    "\n",
    "**Вопрос:** Что мы знаем про $q(x_{t-1}|x_{t})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1dbbf4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Учим модель не ответ - давайте сразу обучим $x \\sim \\pi(x)$ предсказывать\n",
    "2. Когда мы учили модель предсказывать распределения:\n",
    "    - модель предсказывала **параметры**, которые задают распределение\n",
    "    - это были либо нормальные, либо категориальные распределения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf17b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditioned reverse distribution\n",
    "\n",
    "Давайте упростим $q(x_{t-1}|x_{t})$. Как это делали на лекции?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb520c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Подсказка:** Почему у диффузионных моделей долгое семплирование?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f4c7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Feller theorem 1949** - при мальеньких шагах диффузии обратное преобразование тоже будет гауссовским\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240507204248.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ac1e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Вопрос:** Мы по прежнему не можем явно вычислить $q(x_{t-1}|x_t)$. Что еще можно упростить, чтобы явно выписать $\\mu, \\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67e20b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$q(x_{t-1}|x_{t})$ мы посчитать не можем, НО $q(x_{t-1}|x_t, x_0)$ можем!\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240501143147.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0274702",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Вопрос:** В чем обман?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7efb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$q(x_{t-1}|x_{t})$ мы посчитать не можем, НО $q(x_{t-1}|x_t, x_0)$ можем!\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240501143147.png\" width=1000 /></center>\n",
    "\n",
    "**Вопрос:** В чем обман?\n",
    "\n",
    "1. С точки зрения математики ни в чем, все формулы корректны\n",
    "2. *Интуиция* ака *махание руками* - мы из нормального шума можем получить все, что угодно, поэтому непонятно \"куда идти\". Если же знаем конечное $x_0$ куда хотим расшумить, становится чуть проще"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188be40d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Теперь можно параметризовать $q(x_{t-1}|x_t, x_0)$ и учить модели.\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240430204919.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfd522",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Вопрос:** С какой loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1a441",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240507205459.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397722e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VAE recap\n",
    "\n",
    "**А откуда вообще берется ELBO?**\n",
    "\n",
    "5 семинар про VAE \n",
    "\n",
    "> The logic of the process:\n",
    "> 1. Introduce Latent Variable Model (**LVM**).\n",
    "> 2. Derive the Evidence Lower Bound (**ELBO**), notice that $argmax_q L = argmin_q KL[q, p(z|x, \\theta)] = p(z|x, \\theta)$.\n",
    "> 3. **EM algorithm** (tighten the lower bound on ELBO, thereby tightening the likelihood).\n",
    "> 4. Introduce a **parametric** $q(z|x, \\phi)$, transitioning to amortized variational inference (gradient optimization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd43f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Вводим LVM\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240507213823.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98993d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Вводим LVM\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240507213850.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf55a1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Выписываем ELBO\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240507214229.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f7b29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "В конце вывода останется\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240501155653.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc341f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Фактически \"обучение модели\" значит - обучить $p(x_{t-1}|x_{t})$ приближать $q(x_{t-1}|x_{t}, x_0)$\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240501160106.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721239cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Вопрос:** Как почситать $L_t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cf510",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Это $KL$ между двумя нормальными распределениями, при этом $q(x_{t-1}|x_t, x_0)$ мы явно считать умеем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c890323",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Несколько трюков / упрощений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81365c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. **Будем явно считать** $\\sigma_{\\theta,t}$. У распределения которое мы учим 2 параметра - $\\mu, \\sigma$. Есть теоретические оценики на дисперсию, можно посчитать ее аналитически так же, как мы считаем для вычислимого $q(x_{t-1}|x_t, x_0)$. Это сильно упростит вид KL и loss функция будет зависеть только от $\\mu$\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240501203805.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321a38b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. **Репараметризация через $x_t$ и $\\epsilon$**. Мат. ожидание $q(x_{t-1}|x_t, x_0)$ считается через $x_t$ и $x_0$, при этом $x_t$ считается из $x_0$ и $\\epsilon$ $\\Rightarrow$ выразим мат. ожидание через $x_t$ и нормальный шум $\\epsilon$\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240501204235.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65240ac7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. **Вынесем константы.**  Cократим $x_t$, вынесем все константы за скобки и оставить только шум\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240501204436.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61c4f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Получили loss function - \"ну давайте шум предсказывать\"\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240501205916.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cbea9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Итого\n",
    "\n",
    "С точки зрения математики:\n",
    "1. **Forward gaussian diffusion process** - *марково* зашумляем картинку, постепенно размывая исходное изображение  в $N(0, I)$ (\"добавка\", которую подмешиваем $q(x_t|x_{t-1}) \\rightarrow N(0, I)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6bd2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. **Feller theorem**. Нужно научиться считать обратное преобразование. Если мы будем портить картинку \"по чуть-чуть\", то **и обратное преобразование будет гауссово** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1c1e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Чтобы обучить модель \"делать обратное преобразование\" - вводим **LVM**, расписываем **ELBO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529fede",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. ELBO - получаем лосс из 3 слагаемых:\n",
    "\t1. **Reconstruction locc** с маленьким зашумлением, тоже денойзер\n",
    "\t2. $\\mathbf{KL}(p(x_T|x_0)|p(x_T))$ $\\approx 0$ по построению процесса прямой диффузии - все переходит в $N(0, I)$\n",
    "\t3. $\\mathbf{KL}(q(x_{t-1}|x_t, x_0)| p(x_{t-1}|x_t, \\theta))$ между **выученным обратным преобразованием** с и **обратным, которое мы можем посчитать явно** (из-за обусловленности на $x_0$, которая возникла в выводе ELBO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53268df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "На практике: учим модель предсказывать каким шумом зашумили картинку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0f4e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Denoising diffusion propbabilistic model\n",
    "\n",
    "Соберем все вместе\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240501211003.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419c165",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Кто может описать, как происходит **процесс обучения**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7104a4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Замечания:**\n",
    "1. Семплируем $t, \\epsilon$ , считаем $x_t$ через $x_0$ и $\\epsilon$\n",
    "2. Модель предсказывает $\\epsilon$ - шум, который добавили к $x_0$\n",
    "3. Модель обусловлена на $t$ - мы в нее тоже каким-то образом сразу подаем знание о том, для какого $t$ предсказываем шум\n",
    "4. Policy $\\beta$ фактически задает и прямой и обратный диффузионный процесс\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a304e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Кто может описать, как происходит **процесс семплирования**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40ea25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Cемплируем $x_T = \\epsilon$ из $N(0, I)$\n",
    "2. Cчитаем $x_{t-1}$ зная $x_t$\n",
    "\t1. Обученым денойзером предсказываем шум $\\epsilon_{\\theta}(x_t, t)$\n",
    "\t2. Считаем мат. ожидание $\\mu_{\\theta}(x_t, t, \\epsilon_\\theta)$\n",
    "\t3. К $\\mu_{\\theta}(x_t, t, \\epsilon_\\theta)$ добавляем шум $\\epsilon \\sim N(0, I)$ умноженный на дисперсию\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7209952",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240501211039.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9765edb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Вопрос:** А что будет, если не добавлять шум $\\epsilon$ в семплировании?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad8689f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Вопрос:** Какие проблемы есть в текущем сетапе?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83968bdf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Guidance\n",
    "2. Slow sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36b6ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Classifier**\n",
    "\n",
    "Отдельно учим классификатор на **зашумленных данных** (зависимость от $t$ можно прокидывать так же эмбедингами)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240504161234.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d7d99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240504161409.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48a273",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Classifier free**\n",
    "\n",
    "Более часто используемый на практике. Нужно переучивать / учить сразу условную генерацию\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240504161740.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74821e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240504161843.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1252df8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling\n",
    "\n",
    "### Problem 1: Faster sampling with DDPM (spaced diffusion) (1pt)\n",
    "\n",
    "Sampling from DDPM is very slow. In the practical part of HW you will see that it took about 8 seconds to generate a batch of images with our diffusion model (even using a gpu).\n",
    "While, as you might remember, it took less than a second using other generative models (VAE/GAN/NF).\n",
    "This drawback can't be solved generally with using more gpus, since it requires iterative sampling.\n",
    "There are several techniques to alleviate this drawback. In this task We are going to investigate one of them.\n",
    "\n",
    "Assume we have already trained a model $p(\\mathbf{x}_{t - 1} | \\mathbf{x}_t, \\boldsymbol{\\theta})$ to \"reverse\" a Markov chain of length $T$.\n",
    "\n",
    "Let try to build inference process using subsequence of timesteps\n",
    "$\\{S_0 = 0, S_1, \\ldots, S_{T'-1}, S_{T'} = T\\}$, where $T' < T$.\n",
    "\n",
    "Using this subsequence we have to do $T' (< T)$ inference steps instead of $T$. It could dramatically reduce inference time.\n",
    "\n",
    "Diffusion models inference are essentially defined by\n",
    "- schedule of variances $\\{\\beta_1, \\ldots, \\beta_T\\}$\n",
    "- reverse process:\n",
    "$$\n",
    "p(\\mathbf{x}_{S_{t - 1}} | \\mathbf{x}_{S_t}, \\boldsymbol{\\theta}) = \\mathcal{N} \\bigl(\\mathbf{x}_{S_{t - 1}} | \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{S_t}, S_t), \\tilde{\\beta}_{S_t}\\bigr)\n",
    "$$\n",
    "\n",
    "Therefore, all you have to find is the variances for the new Markov chain: $\\{\\tilde{\\beta}_{S_1}, \\ldots, \\tilde{\\beta}_{S_{T'}}\\}$.\n",
    "\n",
    "**Task:** find the expression for $\\tilde{\\beta}_{S_t}$ (it should depend on $\\alpha$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba9823",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
