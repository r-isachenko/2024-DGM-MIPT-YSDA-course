{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf84df9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Generative Models</center>\n",
    "## <center>Seminar 11</center>\n",
    "\n",
    "<center>19.11.2024</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f5923",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plan\n",
    "\n",
    "1. Guidance\n",
    "2. Papers, please\n",
    "    - CLIP\n",
    "    - GLIDE\n",
    "    - DALL-E 2\n",
    "    - Imagen\n",
    "    - Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeab49c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Guidance\n",
    "\n",
    "**Что помните?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f714f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Classifier**\n",
    "\n",
    "Отдельно учим классификатор на **зашумленных данных** (зависимость от $t$ можно прокидывать так же эмбедингами)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240504161234.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16faa32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240504161409.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16ecf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Classifier free**\n",
    "\n",
    "Более часто используемый на практике. Нужно переучивать / учить сразу условную генерацию\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240504161740.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d00c46a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240504161843.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1aa13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CLIP\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2103.00020)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240507010951.png\" width=2000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6bca2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"attachments/Pasted image 20240507011012.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54f45c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"attachments/Pasted image 20240514233700.png\" width=1500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69c8ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GLIDE\n",
    "\n",
    "GOTO [Paper](https://arxiv.org/pdf/2112.10741) and [code](https://github.com/openai/glide-text2im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80069a81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classifier guidance\n",
    "\n",
    "**Training:** Train only classifier on a noised samples\n",
    "\n",
    "**Sampling:** $\\hat{\\mu}_{\\theta}(x_t|y) = \\mu_{\\theta}(x_t|y) + s \\cdot \\Sigma_{\\theta}(x_t|y) \\nabla_{x_t} \\log p_{\\phi}(y|x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1b103",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Classifier-free guidance\n",
    "\n",
    "**Training:** Train $\\epsilon_{\\theta}(x_t|c)$ and $\\epsilon_{\\theta}(x_t|\\varnothing)$\n",
    "\n",
    "**Sampling:** $\\hat{\\epsilon}_{\\theta}(x_t|c) = \\epsilon_{\\theta}(x_t|\\varnothing) + s \\cdot (\\epsilon_{\\theta}(x_t|c) - \\epsilon_{\\theta}(x_t|\\varnothing))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a12d74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515044551.png\" width=1300 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965fdd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515044634.png\" width=800 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2741a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DALL-E 2\n",
    "\n",
    "**Question:** What else does the CLIP model have besides $z_t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e2d696",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** $z_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462608e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DALL-E 2\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2204.06125)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515003818.png\" width=1500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f29bc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Prior is needed\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515015457.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dacd81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Robust $z_i$\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515030935.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ccf85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparison with GLIDE\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515044406.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bbdd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DALL-E 2 Recap\n",
    "\n",
    "**Prior** $P(z_i|y)$: Predicts CLIP image embeddings conditioned on captions.\n",
    "- Autoregressive Model: Initially used, but less effective\n",
    "- Diffusion Model: Preferred for better performance and efficiency\n",
    "\n",
    "**Decoder** $P(x|z_i, y)$: Produces images conditioned **on the predicted CLIP image embeddings** and **captions**.\n",
    "- Generate 64×64 images\n",
    "\n",
    "**Upsample** images from 64×64 $\\rightarrow$ 256×256 $\\rightarrow$ 1024×1024 resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397f00b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imagen\n",
    "\n",
    "**Question:** How else can information from text be encoded and transferred to a generative model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ec068",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** Use LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9a9ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imagen\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/https://arxiv.org/pdf/2205.11487)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515022553.png\" width=1500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7907a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240515033302.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7e733",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240515033825.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957723a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Proof that scaling is important\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515034632.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cdaae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Human raters find Imagen samples to be on par with the COCO data itself in image-text alignment \n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515033121.png\" width=800 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1b60b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "IMHO, the most important plot in the paper\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515034129.png\" width=1500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62494ee6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One more important plot in the paper\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515181057.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3538e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imagen Recap\n",
    "\n",
    "#### Architecture:\n",
    "- **Text Encoder**: Utilizes a pre-trained language model (T5-XXL) for text embeddings.\n",
    "- **Image Decoder**: Employs a diffusion model to generate high-fidelity images (64×64) from text embeddings **condtioned on text embedings** from the Text Encoder.\n",
    "- **Cascaded Diffusion Models**: Upsample images from 64×64 $\\rightarrow$ 256×256 $\\rightarrow$ 1024×1024 resolution.\n",
    "\n",
    "#### Critical Components:\n",
    "- **Text Encoder Size**: Scaling the text encoder size is more impactful than increasing the U-Net size for image quality.\n",
    "- **Dynamic Thresholding**: Essential for achieving better photorealism and text alignment.\n",
    "- **Human Preference**: Human raters prefer T5-XXL over CLIP on DrawBench evaluations.\n",
    "- **Noise Conditioning Augmentation**: Improves CLIP and FID scores by enhancing text conditioning and reducing artifacts.\n",
    "- **Text Conditioning Method**: Using cross-attention over text embeddings outperforms simple pooling methods.\n",
    "- **Efficient U-Net**: Implementation uses less memory, converges faster, and improves sample quality with quicker inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db57263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Diffusion Model\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2112.10752)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240511183056.png\" width=1300 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148624b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240511200618.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c28aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240511200929.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5c527",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LDM Recap\n",
    "\n",
    "**Encoder/Decoder:**\n",
    "- **Training**: Utilizes an autoencoder trained to compress images into a lower-dimensional latent space.\n",
    "- **Regularization**: Employs KL-divergence for slight penalty towards a normal distribution and a vector quantization (VQ) codebook to maintain discrete latent representations.\n",
    "\n",
    "**Diffusion Generator:** \n",
    "- Built on a U-Net structure integrated with **attention mechanisms** allow to use domain-specific encoder $\\tau_{\\theta}$ **conditioning on various modalities**: Semantic Map, Text Embeddings, Images, Binary Inpainting mask.\n",
    "\n",
    "\n",
    "**Multitask Capabilities:**\n",
    "- **Text-to-Image**: Transforms textual descriptions into corresponding high-resolution images using cross-attention to integrate text embeddings.\n",
    "- **Image Inpainting**: Fills in missing or removed parts of images, maintaining contextual coherence of the original image.\n",
    "- **Image-to-Image Translation**: Converts images from one domain to another (e.g., day to night scenes) leveraging the latent space embeddings.\n",
    "- **Super-Resolution**: Enhances the resolution of images from lower to higher resolutions, preserving image quality and detail.\n",
    "- **Unconditional Image Generation**: Generates images without specific input conditions, achieving diversity in output."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
