{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-humidity",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Generative Models</center>\n",
    "## <center>Seminar 13</center>\n",
    "\n",
    "<center><img src=\"pics/AIMastersLogo.png\" width=600 /></center>\n",
    "<center>15.05.2024</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9313cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plan\n",
    "\n",
    "1. Guidance recap\n",
    "2. Papers, please\n",
    "    - CLIP\n",
    "    - GLIDE\n",
    "    - DALL-E 2\n",
    "    - Imagen\n",
    "    - Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36b6ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Classifier**\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240504161234.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d7d99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240504161409.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48a273",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Classifier free**\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240504161740.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74821e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240504161843.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eebf41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CLIP\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2103.00020)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240507010951.png\" width=2000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f192e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"attachments/Pasted image 20240507011012.png\" width=1000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb342e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"attachments/Pasted image 20240514233700.png\" width=1500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b175521",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GLIDE\n",
    "\n",
    "GOTO [Paper](https://arxiv.org/pdf/2112.10741) and [code](https://github.com/openai/glide-text2im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b9ba5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classifier guidance\n",
    "\n",
    "**Training:** Train only classifier on a noised samples\n",
    "\n",
    "**Sampling:** $\\hat{\\mu}_{\\theta}(x_t|y) = \\mu_{\\theta}(x_t|y) + s \\cdot \\Sigma_{\\theta}(x_t|y) \\nabla_{x_t} \\log p_{\\phi}(y|x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15db2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Classifier-free guidance\n",
    "\n",
    "**Training:** Train $\\epsilon_{\\theta}(x_t|c)$ and $\\epsilon_{\\theta}(x_t|\\varnothing)$\n",
    "\n",
    "**Sampling:** $\\hat{\\epsilon}_{\\theta}(x_t|c) = \\epsilon_{\\theta}(x_t|\\varnothing) + s \\cdot (\\epsilon_{\\theta}(x_t|c) - \\epsilon_{\\theta}(x_t|\\varnothing))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74f903",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515044551.png\" width=1300 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e19cf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515044634.png\" width=800 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b91b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DALL-E 2\n",
    "\n",
    "**Question:** What else does the CLIP model have besides $z_t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc98f3f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** $z_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c690e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DALL-E 2\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2204.06125)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515003818.png\" width=1500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ef3c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Prior is needed\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515015457.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0659f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Robust $z_i$\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515030935.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88508d74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparison with GLIDE\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515044406.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f7bbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DALL-E 2 Recap\n",
    "\n",
    "**Prior** $P(z_i|y)$: Predicts CLIP image embeddings conditioned on captions.\n",
    "- Autoregressive Model: Initially used, but less effective\n",
    "- Diffusion Model: Preferred for better performance and efficiency\n",
    "\n",
    "**Decoder** $P(x|z_i, y)$: Produces images conditioned **on the predicted CLIP image embeddings** and **captions**.\n",
    "- Generate 64×64 images\n",
    "\n",
    "**Upsample** images from 64×64 $\\rightarrow$ 256×256 $\\rightarrow$ 1024×1024 resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d567b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imagen\n",
    "\n",
    "**Question:** How else can information from text be encoded and transferred to a generative model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ece09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** Use LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5933771",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imagen\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/https://arxiv.org/pdf/2205.11487)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515022553.png\" width=1500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afda53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240515033302.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fd8de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240515033825.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcde014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Proof that scaling is important\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515034632.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688b4b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Human raters find Imagen samples to be on par with the COCO data itself in image-text alignment \n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515033121.png\" width=800 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125ecc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "IMHO, the most important plot in the paper\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515034129.png\" width=1500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30670872",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One more important plot in the paper\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240515181057.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c24bcf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imagen Recap\n",
    "\n",
    "#### Architecture:\n",
    "- **Text Encoder**: Utilizes a pre-trained language model (T5-XXL) for text embeddings.\n",
    "- **Image Decoder**: Employs a diffusion model to generate high-fidelity images (64×64) from text embeddings **condtioned on text embedings** from the Text Encoder.\n",
    "- **Cascaded Diffusion Models**: Upsample images from 64×64 $\\rightarrow$ 256×256 $\\rightarrow$ 1024×1024 resolution.\n",
    "\n",
    "#### Critical Components:\n",
    "- **Text Encoder Size**: Scaling the text encoder size is more impactful than increasing the U-Net size for image quality.\n",
    "- **Dynamic Thresholding**: Essential for achieving better photorealism and text alignment.\n",
    "- **Human Preference**: Human raters prefer T5-XXL over CLIP on DrawBench evaluations.\n",
    "- **Noise Conditioning Augmentation**: Improves CLIP and FID scores by enhancing text conditioning and reducing artifacts.\n",
    "- **Text Conditioning Method**: Using cross-attention over text embeddings outperforms simple pooling methods.\n",
    "- **Efficient U-Net**: Implementation uses less memory, converges faster, and improves sample quality with quicker inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdee7d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Diffusion Model\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2112.10752)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240511183056.png\" width=1300 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5b03d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240511200618.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09845a79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240511200929.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976d4ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LDM Recap\n",
    "\n",
    "**Encoder/Decoder:**\n",
    "- **Training**: Utilizes an autoencoder trained to compress images into a lower-dimensional latent space.\n",
    "- **Regularization**: Employs KL-divergence for slight penalty towards a normal distribution and a vector quantization (VQ) codebook to maintain discrete latent representations.\n",
    "\n",
    "**Diffusion Generator:** \n",
    "- Built on a U-Net structure integrated with **attention mechanisms** allow to use domain-specific encoder $\\tau_{\\theta}$ **conditioning on various modalities**: Semantic Map, Text Embeddings, Images, Binary Inpainting mask.\n",
    "\n",
    "\n",
    "**Multitask Capabilities:**\n",
    "- **Text-to-Image**: Transforms textual descriptions into corresponding high-resolution images using cross-attention to integrate text embeddings.\n",
    "- **Image Inpainting**: Fills in missing or removed parts of images, maintaining contextual coherence of the original image.\n",
    "- **Image-to-Image Translation**: Converts images from one domain to another (e.g., day to night scenes) leveraging the latent space embeddings.\n",
    "- **Super-Resolution**: Enhances the resolution of images from lower to higher resolutions, preserving image quality and detail.\n",
    "- **Unconditional Image Generation**: Generates images without specific input conditions, achieving diversity in output."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
