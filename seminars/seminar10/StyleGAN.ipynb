{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN\n",
    "\n",
    "**Credits**: \n",
    "\n",
    "The code is based on [github_1](https://github.com/davidsosa/GANs) and [github_2](https://github.com/rosinality/style-based-gan-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    GPU_DEVICE = 2\n",
    "    FULL_DEVICE = 'cuda:{}'.format(GPU_DEVICE)\n",
    "    torch.cuda.set_device(GPU_DEVICE)\n",
    "else:\n",
    "    DEVICE='cpu'\n",
    "# DEVICE='cpu'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reset_seed():\n",
    "    OUTPUT_SEED = 0xBADBEEF\n",
    "    #OUTPUT_SEED = 42\n",
    "    torch.manual_seed(OUTPUT_SEED)\n",
    "    np.random.seed(OUTPUT_SEED)\n",
    "\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../homeworks') # to grab dgm_utils from ../../homeworks directory\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dgm_utils import train_model, show_samples, visualize_images\n",
    "from dgm_utils import visualize_2d_samples, visualize_2d_densities, visualize_2d_data\n",
    "# from dgm_utils import load_pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.distributions as TD\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images,\n",
    "    size per image, and images per row, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu().clamp_(0, 1)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow, padding=0)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mapping Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MappingLayers(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    Mapping Layers Class\n",
    "    Values:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
    "    '''\n",
    " \n",
    "    def __init__(self, z_dim, hidden_dim, w_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            # A neural network which takes in tensors of \n",
    "            # shape (n_samples, z_dim) and outputs (n_samples, w_dim)\n",
    "            # with a hidden layer with hidden_dim neurons\n",
    "            nn.Linear(z_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, w_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of MappingLayers: \n",
    "        Given an initial noise tensor, returns the intermediate noise tensor.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
    "        '''\n",
    "        return self.mapping(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Truncation trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "def get_truncated_noise(n_samples, z_dim, truncation):\n",
    "    '''\n",
    "    Function for creating truncated noise vectors: Given the dimensions (n_samples, z_dim)\n",
    "    and truncation value, creates a tensor of that shape filled with random\n",
    "    numbers from the truncated normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        truncation: the truncation value, a non-negative scalar\n",
    "    '''\n",
    "    truncated_noise = truncnorm.rvs(-1*truncation, truncation, size=(n_samples, z_dim))\n",
    "    return torch.Tensor(truncated_noise)\n",
    "\n",
    "def scale_w(w, w_mean=None, w_weight=None):\n",
    "    if w_mean is not None:\n",
    "        w = w_mean + w_weight * (w - w_mean)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Injection Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InjectNoise(nn.Module):\n",
    "    '''\n",
    "    Inject Noise Class\n",
    "    Values:\n",
    "        channels: the number of channels the image has, a scalar\n",
    "    '''\n",
    "    def __init__(self, channels):\n",
    "\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter( # You use nn.Parameter so that these weights can be optimized\n",
    "            # Initiate the weights for the channels from a random normal distribution\n",
    "            torch.randn(channels)[None, :, None, None] #torch.randn((1,channels,1,1))\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of InjectNoise: Given an image, \n",
    "        returns the image with random noise added.\n",
    "        Parameters:\n",
    "            image: the feature map of shape (n_samples, channels, width, height)\n",
    "        '''\n",
    "        # Set the appropriate shape for the noise!\n",
    "\n",
    "        noise_shape = (image.shape[0], 1, image.shape[2], image.shape[3])\n",
    "\n",
    "        noise = torch.randn(noise_shape, device=image.device) # Creates the random noise\n",
    "        return image + self.weight * noise # Applies to image after multiplying by the weight for each channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adaptive Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adaptive Instance Norm\n",
    "class AdaIN(nn.Module):\n",
    "    '''\n",
    "    AdaIN Class\n",
    "    Values:\n",
    "        channels: the number of channels the image has, a scalar\n",
    "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Normalize the input per-channels\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "\n",
    "        # You want to map w to a set of style weights per channel.\n",
    "        # Replace the Nones with the correct dimensions - keep in mind that \n",
    "        # both linear maps transform a w vector into style weights \n",
    "        # corresponding to the number of image channels.\n",
    "        self.style_scale_transform = nn.Linear(w_dim, channels)\n",
    "        self.style_shift_transform = nn.Linear(w_dim, channels)\n",
    "\n",
    "    def forward(self, image, w):\n",
    "        '''\n",
    "        Function for completing a forward pass of AdaIN: Given an image and intermediate noise vector w, \n",
    "        returns the normalized image that has been scaled and shifted by the style.\n",
    "        Parameters:\n",
    "            image: the feature map of shape (n_samples, channels, width, height)\n",
    "            w: the intermediate noise vector w\n",
    "        '''\n",
    "        normalized_image = self.instance_norm(image) # (bs, c, w, h)\n",
    "        style_scale = self.style_scale_transform(w)[:, :, None, None]\n",
    "        style_shift = self.style_shift_transform(w)[:, :, None, None]\n",
    "\n",
    "        # Calculate the transformed image\n",
    "        transformed_image = style_scale * normalized_image + style_shift\n",
    "        return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Progressive GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MicroStyleGANGeneratorBlock(nn.Module):\n",
    "    '''\n",
    "    Micro StyleGAN Generator Block Class\n",
    "    Values:\n",
    "        in_chan: the number of channels in the input, a scalar\n",
    "        out_chan: the number of channels wanted in the output, a scalar\n",
    "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
    "        kernel_size: the size of the convolving kernel\n",
    "        starting_size: the size of the starting image\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_chan, \n",
    "        out_chan, \n",
    "        w_dim, \n",
    "        kernel_size, \n",
    "        starting_size, \n",
    "        use_upsample=True):\n",
    "\n",
    "        super().__init__()\n",
    "        self.use_upsample = use_upsample\n",
    "\n",
    "        if self.use_upsample:\n",
    "            self.upsample = nn.Upsample(starting_size, mode='bilinear')\n",
    "        self.conv = nn.Conv2d(in_chan, out_chan, kernel_size, padding=kernel_size // 2) # Padding is used to maintain the image size\n",
    "        self.inject_noise = InjectNoise(out_chan)\n",
    "        self.adain = AdaIN(out_chan, w_dim)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        '''\n",
    "        Function for completing a forward pass of MicroStyleGANGeneratorBlock: Given an x and w, \n",
    "        computes a StyleGAN generator block.\n",
    "        Parameters:\n",
    "            x: the input into the generator, feature map of shape (n_samples, channels, width, height)\n",
    "            w: the intermediate noise vector\n",
    "        '''\n",
    "        if self.use_upsample:\n",
    "            x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.inject_noise(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.adain(x, w)\n",
    "        return x\n",
    "\n",
    "class StyledSequential(nn.Sequential):\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MicroStyleGANGenerator(nn.Module):\n",
    "    '''\n",
    "    Micro StyleGAN Generator Class\n",
    "    Values:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        map_hidden_dim: the mapping inner dimension, a scalar\n",
    "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
    "        in_chan: the dimension of the constant input, usually w_dim, a scalar\n",
    "        out_chan: the number of channels wanted in the output, a scalar\n",
    "        kernel_size: the size of the convolving kernel\n",
    "        hidden_chan: the inner dimension, a scalar\n",
    "    '''\n",
    "\n",
    "    def _sample_prior(self, n):\n",
    "        x = self.prior.sample((n, self.z_dim))\n",
    "        return x\n",
    "\n",
    "    def __init__(self, \n",
    "                 z_dim, # z dimensionality\n",
    "                 map_hidden_dim, # mapping network parameter\n",
    "                 w_dim, # style vector dimensionality\n",
    "                 in_chan, # number of channels in input trainable tensor\n",
    "                 out_chan, # images dimensionality\n",
    "                 kernel_size, \n",
    "                 hidden_chan, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.prior = TD.Normal(torch.tensor(0.0).to(device), torch.tensor(1.0).to(device))\n",
    "        self.z_dim = z_dim\n",
    "        self.map = MappingLayers(z_dim, map_hidden_dim, w_dim)\n",
    "        # Typically this constant is initiated to all ones, but you will initiate to a\n",
    "        # Gaussian to better visualize the network's effect\n",
    "        self.starting_constant = nn.Parameter(torch.randn(1, in_chan, 4, 4))\n",
    "\n",
    "        self.progression = nn.ModuleList(\n",
    "            [\n",
    "                StyledSequential(*[\n",
    "                    MicroStyleGANGeneratorBlock(in_chan, hidden_chan, w_dim, kernel_size, 4, use_upsample=False),\n",
    "                    MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 4, use_upsample=False)]),\n",
    "                StyledSequential(*[\n",
    "                    MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 8, use_upsample=True),\n",
    "                    MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 8, use_upsample=False)]),\n",
    "                StyledSequential(*[\n",
    "                    MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 16, use_upsample=True),\n",
    "                    MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 16, use_upsample=False)]),\n",
    "                StyledSequential(*[\n",
    "                    MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 32, use_upsample=True),\n",
    "                    MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 32, use_upsample=False)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.to_rgb = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(*[nn.Conv2d(hidden_chan, out_chan, kernel_size=1), nn.Tanh()]), # out_chan = 3\n",
    "                nn.Sequential(*[nn.Conv2d(hidden_chan, out_chan, kernel_size=1), nn.Tanh()]),\n",
    "                nn.Sequential(*[nn.Conv2d(hidden_chan, out_chan, kernel_size=1), nn.Tanh()]),\n",
    "                nn.Sequential(*[nn.Conv2d(hidden_chan, out_chan, kernel_size=1), nn.Tanh()])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, step=0, alpha=-1, w_mean=None, w_weight=None):\n",
    "        '''\n",
    "        Function for completing a forward pass of MicroStyleGANGenerator: Given noise, \n",
    "        computes a StyleGAN iteration.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
    "            return_intermediate: a boolean, true to return the images as well (for testing) and false otherwise\n",
    "        '''\n",
    "        x = self.starting_constant\n",
    "        w = self.map(noise)\n",
    "        w = scale_w(w, w_mean, w_weight)\n",
    "\n",
    "        for i, (conv, to_rgb) in enumerate(zip(self.progression, self.to_rgb)):\n",
    "\n",
    "            if i > 0 and step > 0:\n",
    "                x_prev = x\n",
    "\n",
    "            x = conv(x, w)\n",
    "\n",
    "            if i == step:\n",
    "                x = to_rgb(x)\n",
    "\n",
    "                if i > 0 and 0 <= alpha < 1:\n",
    "                    skip_rgb = self.to_rgb[i - 1](x_prev)\n",
    "                    skip_rgb = F.interpolate(skip_rgb, scale_factor=2, mode='bilinear')\n",
    "                    x = (1 - alpha) * skip_rgb + alpha * x\n",
    "\n",
    "                break\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample(self, n, step=0, alpha=-1, w_mean=None, w_weight=None):\n",
    "        with torch.no_grad():\n",
    "            return self.rsample(n, step, alpha, w_mean, w_weight)\n",
    "\n",
    "    def rsample(self, n, step=0, alpha=-1, w_mean=None, w_weight=None):\n",
    "        noise = self._sample_prior(n)\n",
    "        return self.forward(noise, step, alpha, w_mean, w_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_dim = 128\n",
    "out_chan = 3\n",
    "truncation = 0.7\n",
    "\n",
    "SG_generator = MicroStyleGANGenerator(\n",
    "    z_dim=z_dim, \n",
    "    map_hidden_dim=256,\n",
    "    w_dim=64,\n",
    "    in_chan=64,\n",
    "    out_chan=out_chan, \n",
    "    kernel_size=3, \n",
    "    hidden_chan=32,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "test_samples = 10\n",
    "test_result = SG_generator(get_truncated_noise(test_samples, z_dim, truncation), step=3)\n",
    "\n",
    "# Check if the block works\n",
    "assert tuple(test_result.shape) == (test_samples, out_chan, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SG_generator.sample(10, step=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "\n",
    "viz_samples = 10\n",
    "# The noise is exaggerated for visual effect\n",
    "viz_noise = get_truncated_noise(viz_samples, z_dim, truncation) * 10\n",
    "\n",
    "SG_generator.eval()\n",
    "images = []\n",
    "for alpha in np.linspace(0, 1, num=5):\n",
    "    viz_result =  SG_generator(viz_noise, step=1, alpha=alpha)\n",
    "    images += [tensor for tensor in viz_result]\n",
    "show_tensor_images(torch.stack(images), nrow=viz_samples, num_images=len(images))\n",
    "SG_generator = SG_generator.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        kernel_size,\n",
    "        downsample=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size, padding=kernel_size//2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        if downsample:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(out_channel, out_channel, kernel_size, padding=kernel_size//2),\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(out_channel, out_channel, kernel_size, padding=kernel_size//2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_chan,\n",
    "        out_chan, \n",
    "        kernel_size, \n",
    "        hidden_chan):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.progression = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(hidden_chan, hidden_chan, kernel_size, downsample=True), # 16\n",
    "                ConvBlock(hidden_chan, hidden_chan, kernel_size, downsample=True), # 8\n",
    "                ConvBlock(hidden_chan, hidden_chan, kernel_size, downsample=True), # 4\n",
    "                nn.Sequential(*[\n",
    "                    nn.Conv2d(hidden_chan, hidden_chan, kernel_size, padding=kernel_size//2),\n",
    "                    nn.LeakyReLU(0.2),\n",
    "                    nn.Conv2d(hidden_chan, in_chan, 4, padding=0),\n",
    "                    nn.LeakyReLU(0.2)])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.from_rgb = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(out_chan, hidden_chan, kernel_size=1),\n",
    "                nn.Conv2d(out_chan, hidden_chan, kernel_size=1),\n",
    "                nn.Conv2d(out_chan, hidden_chan, kernel_size=1),\n",
    "                nn.Conv2d(out_chan, hidden_chan, kernel_size=1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.n_layer = len(self.progression)\n",
    "\n",
    "        self.linear = nn.Linear(in_chan, 1)\n",
    "\n",
    "    def forward(self, input, step=0, alpha=-1):\n",
    "        out = self.from_rgb[self.n_layer - step - 1](input)\n",
    "        out = self.progression[self.n_layer - step - 1](out)\n",
    "\n",
    "        if step > 0 and  0 <= alpha < 1:\n",
    "            skip_rgb = F.avg_pool2d(input, 2)\n",
    "            skip_rgb = self.from_rgb[self.n_layer - step](skip_rgb)\n",
    "            out = (1 - alpha) * skip_rgb + alpha * out\n",
    "\n",
    "        for i in range(step - 1, -1, -1):\n",
    "            index = self.n_layer - i - 1\n",
    "            out = self.progression[index](out)\n",
    "\n",
    "        out = out.squeeze(2).squeeze(2)\n",
    "        # print(input.size(), out.size(), step)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SG_discriminator = Discriminator(\n",
    "    in_chan=64,\n",
    "    out_chan=out_chan, \n",
    "    kernel_size=3, \n",
    "    hidden_chan=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viz_result =  SG_generator(viz_noise, step=2, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 16, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discr_results = SG_discriminator(viz_result, step=2, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discr_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dgm_utils import load_dataset\n",
    "train_data, test_data = load_dataset('cifar10', binarize=False)\n",
    "visualize_images(train_data, 'CIFAR10 samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import skimage.measure\n",
    "\n",
    "class MultiResCIFAR10Dataset(Dataset):\n",
    "\n",
    "    res2avgpoolops = {\n",
    "        32: 0,\n",
    "        16: 1,\n",
    "        8: 2,\n",
    "        4: 3,\n",
    "        2: 4\n",
    "    }\n",
    "\n",
    "    @property\n",
    "    def resolution(self):\n",
    "        return self._resolution\n",
    "\n",
    "    @resolution.setter\n",
    "    def resolution(self, val):\n",
    "        assert val in [2, 4, 8, 16, 32]\n",
    "        self._resolution = val\n",
    "        self._avgpoolops = self.res2avgpoolops[val]\n",
    "        if val in self.data.keys():\n",
    "            return\n",
    "        self.data[val] = np.asarray([\n",
    "            self.np_avgpool2(self.transform(self.init_data[i]), self._avgpoolops) for i in range(len(self.init_data))])\n",
    "\n",
    "    def np_avgpool2(self, x, num_apply):\n",
    "        for _ in range(num_apply):\n",
    "            x = skimage.measure.block_reduce(x, (1, 2, 2), np.mean)\n",
    "        return x\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data,\n",
    "        resolution=32,\n",
    "        transform=lambda x: x,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_data = data\n",
    "        self.data = {}\n",
    "        self.transform = transform\n",
    "        self.resolution = resolution\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.init_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[self.resolution][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def im2rawim(x):\n",
    "    return (x - 0.5)/0.5\n",
    "\n",
    "def rawim2im(x):\n",
    "    return x/2. + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = MultiResCIFAR10Dataset(train_data, transform = im2rawim, resolution=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resolutions = [32, 16, 8, 4]\n",
    "for res in resolutions:\n",
    "    train_dataset.resolution = res\n",
    "    dl = DataLoader(train_dataset, batch_size=7, shuffle=False)\n",
    "    batch = next(iter(dl)).numpy() * 0.5 + 0.5\n",
    "    show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(res), figsize=(10, 5), nrow=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Basic Training infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of standard `GAN` loss with $R_1$ regularizer ([paper](https://arxiv.org/pdf/1801.04406.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_defaultdictlist():\n",
    "    return defaultdict(list)\n",
    "\n",
    "class ImagesManager:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stats = defaultdict(get_defaultdictlist)\n",
    "\n",
    "    def add(self, epoch, images):\n",
    "        assert isinstance(images, np.ndarray)\n",
    "        assert len(images.shape) == 4 # bs, c, h, w\n",
    "        res = images.shape[-1]\n",
    "        self.stats[res][epoch].append(images)\n",
    "\n",
    "    def get_resolutions(self):\n",
    "        return list(self.stats.keys())\n",
    "\n",
    "    def get_epochs(self, res):\n",
    "        return sorted(self.stats[res].keys())\n",
    "\n",
    "    def get_images(self, res, epoch):\n",
    "        return self.stats[res][epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seminar10_utils import computePotGrad, StatsManager\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def train_GAN_R1(\n",
    "    generator, # generator model\n",
    "    critic, # critic model\n",
    "    step, # number of progressive steps (defines the resolution under consideration)\n",
    "    alpha_sheduler, # sheduler of interventions\n",
    "    train_loader, # loader of the train data\n",
    "    critic_steps, # number of critic steps per one generator steps\n",
    "    batch_size,\n",
    "    n_epochs,\n",
    "    gen_lr, \n",
    "    critic_lr,\n",
    "    SM, # stats manager to store losses\n",
    "    IM, # images manager to store images\n",
    "    start_epoch = 0,\n",
    "    gp_weight=10, # coefficient of R1 gradient penalty\n",
    "    use_cuda=False,\n",
    "    visualize_steps=10,\n",
    "    save_steps=200\n",
    "):\n",
    "\n",
    "    critic.train()\n",
    "    generator.train()\n",
    "\n",
    "    gen_optimizer = torch.optim.Adam(generator.parameters(), lr=gen_lr, betas=(0, 0.9))\n",
    "    critic_optimizer = torch.optim.Adam(critic.parameters(), lr=critic_lr, betas=(0, 0.9))\n",
    "\n",
    "    curr_iter = 0\n",
    "    d_loss, g_loss = torch.zeros(1), torch.zeros(1)\n",
    "\n",
    "    for epoch_i in tqdm(range(start_epoch, n_epochs + start_epoch)):\n",
    "        for batch_i, real_data in enumerate(train_loader):\n",
    "            curr_iter += 1\n",
    "            alpha = alpha_sheduler(curr_iter)\n",
    "            if use_cuda:\n",
    "                real_data = real_data.cuda()\n",
    "\n",
    "            ################\n",
    "            # CRITIC UPDATE\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            real_data.requires_grad_() # for R1 regularizer estimation\n",
    "            real_scores = critic(real_data, step=step, alpha=alpha)\n",
    "            real_predict = F.softplus(- real_scores).mean()\n",
    "\n",
    "            # R1 gradient penalty\n",
    "            gradients = computePotGrad(real_data, real_scores)\n",
    "            grad_penalty = (\n",
    "                gradients.view(gradients.size(0), -1).norm(2, dim=1) ** 2\n",
    "            ).mean()\n",
    "\n",
    "            fake_data = generator.sample(real_data.shape[0], step=step, alpha=alpha)\n",
    "            fake_scores = critic(fake_data, step=step, alpha=alpha)\n",
    "            fake_predict = F.softplus(fake_scores).mean()\n",
    "\n",
    "            d_loss = fake_predict + real_predict + gp_weight * grad_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            SM.upd('D_loss', d_loss.item())\n",
    "            SM.upd('GP', grad_penalty.item())\n",
    "            SM.upd('alpha', alpha)\n",
    "\n",
    "            ###############\n",
    "            # GENERATOR_UPDATE\n",
    "\n",
    "            if curr_iter % critic_steps == 0:\n",
    "                gen_optimizer.zero_grad()\n",
    "                fake_data = generator.rsample(batch_size, step=step, alpha=alpha)\n",
    "                fake_scores = critic(fake_data, step=step, alpha=alpha)\n",
    "                g_loss = F.softplus(-fake_scores).mean()\n",
    "                g_loss.backward()\n",
    "                gen_optimizer.step()\n",
    "                SM.upd('G_loss', g_loss.item())\n",
    "\n",
    "        SM.upd('step', step)\n",
    "\n",
    "        #################\n",
    "        # statistics collection\n",
    "        # and visualization\n",
    "\n",
    "        if visualize_steps and epoch_i % visualize_steps == 0:\n",
    "            clear_output(wait=True)\n",
    "            print('Epoch {}'.format(epoch_i))\n",
    "            fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "            SM.draw(axes)\n",
    "            plt.tight_layout()\n",
    "            plt.show(); plt.close(fig)\n",
    "\n",
    "            batch = rawim2im(SG_generator.sample(10, step=step, alpha=alpha).detach().cpu().numpy())\n",
    "            show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(res), figsize=(10, 10), nrow=5)\n",
    "            IM.add(epoch_i, batch)\n",
    "\n",
    "        if save_steps and epoch_i % save_steps == save_steps - 1:\n",
    "            torch.save(\n",
    "                {\n",
    "                    'generator': SG_generator.state_dict(),\n",
    "                    'discriminator': SG_discriminator.state_dict(),\n",
    "                },\n",
    "                'checkpoints_gan_r1/gan_r1_loss_step_{}_epoch_{}_critic_lr_{:.5f}_gen_lr_{:.5f}_critic_steps_{}_gp_{}.pth'.format(\n",
    "                    step, epoch_i, critic_lr, gen_lr, critic_steps, gp_weight)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Alpha sheduler\n",
    "\n",
    "Controls the intervention of the low resolution when training with higher resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AlphaShedulerGeneric:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, it):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ConstantAlphaSheduler(AlphaShedulerGeneric):\n",
    "\n",
    "    def __init__(self, alpha=1.):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, it):\n",
    "        return self.alpha\n",
    "\n",
    "class LinearProgressiveAlphaSheduler(AlphaShedulerGeneric):\n",
    "\n",
    "    def __init__(self, n_growing):\n",
    "        super().__init__()\n",
    "        assert n_growing >= 1\n",
    "        self.n_growing = n_growing\n",
    "\n",
    "    def __call__(self, it):\n",
    "        return min(float(it)/self.n_growing, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1  Set up generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_dim = 128\n",
    "out_chan = 3\n",
    "\n",
    "SG_generator = MicroStyleGANGenerator(\n",
    "    z_dim=z_dim, \n",
    "    map_hidden_dim=256,\n",
    "    w_dim=64,\n",
    "    in_chan=64,\n",
    "    out_chan=out_chan, \n",
    "    kernel_size=3, \n",
    "    hidden_chan=32\n",
    ")#.cuda()\n",
    "\n",
    "SG_discriminator = Discriminator(\n",
    "    in_chan=64,\n",
    "    out_chan=out_chan, \n",
    "    kernel_size=3, \n",
    "    hidden_chan=32\n",
    ")#.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "CRITIC_STEPS = 1\n",
    "CRITIC_LR=1e-3\n",
    "GEN_LR=1e-3\n",
    "GP_WEIGHT = 1.\n",
    "\n",
    "VARY_PROGRESSIVE_PARAMS = {\n",
    "    0: {\n",
    "        'N_EPOCHS': 200,\n",
    "        'ALPHA_SCHEDULER': ConstantAlphaSheduler(1.),\n",
    "        'SAVE_STEPS': 200},\n",
    "    1: {\n",
    "        'N_EPOCHS': 200,\n",
    "        'ALPHA_SCHEDULER': LinearProgressiveAlphaSheduler(6000),\n",
    "        'SAVE_STEPS': 200},\n",
    "    2: {\n",
    "        'N_EPOCHS': 300,\n",
    "        'ALPHA_SCHEDULER': LinearProgressiveAlphaSheduler(6000),\n",
    "        'SAVE_STEPS': 300},\n",
    "    3: {\n",
    "        'N_EPOCHS': 600,\n",
    "        'ALPHA_SCHEDULER': LinearProgressiveAlphaSheduler(6000),\n",
    "        'SAVE_STEPS': 200}\n",
    "}\n",
    "\n",
    "SM = StatsManager('D_loss', 'G_loss', 'GP', 'alpha', 'step')\n",
    "IM = ImagesManager()\n",
    "\n",
    "step2res = {\n",
    "    0: 4,\n",
    "    1: 8,\n",
    "    2: 16,\n",
    "    3: 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Image Manager from the complete training\n",
    "#with open('checkpoints_gan_r1/im_manager.pkl', 'rb') as filehandler:\n",
    "#     IM = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# StatsManager from the complete training\n",
    "#with open('checkpoints_gan_r1/stats_manager.pkl', 'rb') as filehandler:\n",
    "#     SM = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# complete training\n",
    "\n",
    "for STEP in [0, 1, 2, 3]:\n",
    "    \n",
    "    # create dataloder\n",
    "    train_dataset.resolution = step2res[STEP]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # launch training\n",
    "    train_GAN_R1(\n",
    "        SG_generator, \n",
    "        SG_discriminator,\n",
    "        STEP,\n",
    "        VARY_PROGRESSIVE_PARAMS[STEP]['ALPHA_SCHEDULER'],\n",
    "        train_loader,\n",
    "        critic_steps=CRITIC_STEPS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        n_epochs=VARY_PROGRESSIVE_PARAMS[STEP]['N_EPOCHS'],\n",
    "        gen_lr=GEN_LR,\n",
    "        critic_lr=CRITIC_LR,\n",
    "        SM=SM,\n",
    "        IM=IM,\n",
    "        gp_weight=GP_WEIGHT,\n",
    "        use_cuda=False,\n",
    "        visualize_steps = 20,\n",
    "        save_steps=VARY_PROGRESSIVE_PARAMS[STEP]['SAVE_STEPS']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization and discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на семплы из модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Image Manager from the complete training\n",
    "with open('checkpoints_gan_r1/im_manager.pkl', 'rb') as filehandler:\n",
    "     IM = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatsManager from the complete training\n",
    "with open('checkpoints_gan_r1/stats_manager.pkl', 'rb') as filehandler:\n",
    "     SM = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoints_gan_r1/gan_r1_loss_step_3_critic_lr_0.00100_gen_lr_0.00100_critic_steps_1_gp_1.0_FINAL.pth', map_location=FULL_DEVICE)\n",
    "SG_generator.load_state_dict(checkpoint['generator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = rawim2im(SG_generator.sample(100, step=3).detach().cpu().numpy())\n",
    "show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(32), figsize=(10, 10), nrow=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на лоссы в ходе обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "SM.draw(axes)\n",
    "plt.tight_layout()\n",
    "plt.show(); plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('checkpoints_gan_r1/im_manager.pkl', 'wb') as filehandler:\n",
    "#     pickle.dump(IM , filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('checkpoints_gan_r1/stats_manager.pkl', 'wb') as filehandler:\n",
    "#     pickle.dump(SM_ , filehandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на семплы в ходе обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for res in IM.get_resolutions():\n",
    "    for epoch in IM.get_epochs(res):\n",
    "        batch = IM.get_images(res, epoch)[0]\n",
    "        show_samples(batch, 'SHAPE ({0}, {0}), EPOCH {1}'.format(res, epoch), figsize=(15, 15), nrow=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `w_mean` interpolation\n",
    "\n",
    "Computation of `w_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    noise = SG_generator._sample_prior(10000)\n",
    "    w = SG_generator.map(noise)\n",
    "    w_mean = w.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = rawim2im(SG_generator.sample(100, step=3, w_mean=w_mean, w_weight=1.0).detach().cpu().numpy())\n",
    "show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(32), figsize=(10, 10), nrow=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Alternative versions of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 GAN with $R_1$ (more training iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "SG_generator_alt = MicroStyleGANGenerator(\n",
    "    z_dim=z_dim, \n",
    "    map_hidden_dim=256,\n",
    "    w_dim=64,\n",
    "    in_chan=64,\n",
    "    out_chan=out_chan, \n",
    "    kernel_size=3, \n",
    "    hidden_chan=32\n",
    ")#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('checkpoints_old/gan_loss_step_3_critic_lr_0.00100_gen_lr_0.00100_critic_steps_1_gp_1.0_v2_FINAL.pth', map_location=FULL_DEVICE)\n",
    "SG_generator_alt.load_state_dict(checkpoint['generator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of `w_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    noise = SG_generator_alt._sample_prior(10000)\n",
    "    w = SG_generator_alt.map(noise)\n",
    "    w_mean_alt = w.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = rawim2im(SG_generator_alt.sample(100, step=3, w_mean=w_mean_alt, w_weight=1.0).detach().cpu().numpy())\n",
    "show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(32), figsize=(10, 10), nrow=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "SG_generator_wgan = MicroStyleGANGenerator(\n",
    "    z_dim=z_dim, \n",
    "    map_hidden_dim=256,\n",
    "    w_dim=64,\n",
    "    in_chan=64,\n",
    "    out_chan=out_chan, \n",
    "    kernel_size=3, \n",
    "    hidden_chan=32\n",
    ")#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\n",
    "    'checkpoints_old/wgan_tanh_loss_step_3_alpha_linear_6000_critic_lr_0.00070_gen_lr_0.00070_critic_steps_5_gp_1.0_FINAL.pth', map_location=FULL_DEVICE)\n",
    "SG_generator_wgan.load_state_dict(checkpoint['generator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of `w_mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    noise = SG_generator_wgan._sample_prior(10000)\n",
    "    w = SG_generator_wgan.map(noise)\n",
    "    w_mean_wgangp = w.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = rawim2im(SG_generator_wgan.sample(100, step=3, w_mean=w_mean_wgangp, w_weight=1.0).detach().cpu().numpy())\n",
    "show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(32), figsize=(10, 10), nrow=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. StyleGAN features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 Style variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_styled(model, ws):\n",
    "    assert len(ws) == 4\n",
    "    with torch.no_grad():\n",
    "        x = model.starting_constant\n",
    "        for conv, w in zip(model.progression, ws):\n",
    "            x = conv(x, w)\n",
    "        return model.to_rgb[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed()\n",
    "with torch.no_grad():\n",
    "    noise = SG_generator_alt._sample_prior(200)\n",
    "    ws_basic = SG_generator_alt.map(noise).view(2, 100, -1)\n",
    "    ws_map = {}\n",
    "    for i in range(2):\n",
    "        ws = ws_basic.clone()\n",
    "        ws[i] = ws[i][0].unsqueeze(0).repeat_interleave(ws_basic.size(1), 0)\n",
    "        ws_map[i] = [ws[0], ws[0], ws[1], ws[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix `coarse`-grained, vary `fine`-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = rawim2im(generate_styled(SG_generator_alt, ws_map[1]).detach().cpu().numpy())\n",
    "show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(32), figsize=(10, 10), nrow=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary `coarse`-grained, fix `fine`-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = rawim2im(generate_styled(SG_generator_alt, ws_map[0]).detach().cpu().numpy())\n",
    "show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(32), figsize=(10, 10), nrow=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Playing with interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsr_linspace(a, b, num):\n",
    "    res = torch.zeros_like(a).unsqueeze(0).repeat_interleave(num + 1, 0)\n",
    "    for i in range(num + 1):\n",
    "        res[i] = a + i * (b - a)/num\n",
    "    return res\n",
    "\n",
    "#reset_seed()\n",
    "with torch.no_grad():\n",
    "    noise = SG_generator_alt._sample_prior(4 * 20)\n",
    "    ws_borders = SG_generator_alt.map(noise).view(20, 2, 2, -1)[19]\n",
    "    ws_basic_linspace = tsr_linspace(ws_borders[0], ws_borders[1], 19).transpose(0, 1) # (2, 20, w_shape)\n",
    "\n",
    "    ws_map = {}\n",
    "    for i in range(2):\n",
    "        ws = ws_basic_linspace.clone()\n",
    "        ws[i] = ws[i][-1].unsqueeze(0).repeat_interleave(ws_basic_linspace.size(1), 0)\n",
    "        ws_map[i] = [ws[0], ws[0], ws[1], ws[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix `coarse`-grained, interpolate `fine`-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = rawim2im(generate_styled(SG_generator_alt, ws_map[1]).detach().cpu().numpy())\n",
    "show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(32), figsize=(20, 20), nrow=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate `coarse`-grained, fix `fine`-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = rawim2im(generate_styled(SG_generator_alt, ws_map[0]).detach().cpu().numpy())\n",
    "show_samples(batch, 'CIFAR10 samples, shape = ({0}, {0})'.format(32), figsize=(20, 20), nrow=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
