{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "1. Problems of common Score Matching approaches\n",
    "\n",
    "    1. The manifold hypothesis\n",
    "\n",
    "    2. Ill-estimated score in low data density regions\n",
    "\n",
    "    3. Slow Langevin Dynamics convergence\n",
    "\n",
    "2. Solution: NCSN \n",
    "\n",
    "    1. Main idea: perturbing data expanding support of our distribution\n",
    "\n",
    "    2. Objective\n",
    "\n",
    "    3. Annealed Langevin dynamics\n",
    "\n",
    "    4. How to choose hyperparameters?\n",
    "\n",
    "3. Apply NCSN to toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as TD\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, clear_output\n",
    "from typing import Callable, Dict, List, Tuple, Union, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems of common Score Matching approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The manifold hypothesis\n",
    "\n",
    "It states that data in the real world tend to concentrate on low dimensional manifolds embedded in a high dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ill-estimated score in low data density regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Langevin Dynamics:**\n",
    "$$\n",
    "    x_t = x_{t - 1} +\n",
    "    \\frac{\\epsilon}{2} \\nabla_x \\log\\pi(x_{t - 1}) +\n",
    "    \\sqrt{\\epsilon} z_t,\n",
    "    \\qquad\n",
    "    z_t \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "To sample from $\\pi(x)$ we have to estimate $\\nabla_x \\log\\pi(x_{t - 1})$ on whole $\\mathbb{R}^D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score Matching objective:**\n",
    "$$\n",
    "    J_{ESM}(\\theta) = \\mathbb{E}_{\\pi(x)} || s_\\theta (x) - \\nabla_x \\log \\pi (x) ||^2\n",
    "$$\n",
    "\n",
    "Pay your attention to the fact, that we estimate the score-function only nearby to our dataset,\n",
    "which according to Manifold Hypothesis lies on a low dimensional manifold.\n",
    "\n",
    "That means, that our trained score-model is ill-estimated at most of the $\\mathbb{R}^D$.\n",
    "\n",
    "As a result, starting from random point and using Langevin Dynamics we are highly probable to fail to converge to the given distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pics/ill_estimated_score.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"pics/ill_estimated_score.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow Langevin Dynamics convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When two modes of the data distribution are separated by low density reegions, Langevin Dymamics will not be able to correctly recover the relative weights of these two modes in reasonable time, and therefore might not converge to the true distribution.\n",
    "\n",
    "Consider a mixture distribution\n",
    "$p_{data}(x) = \\pi p_1(x) + (1 - \\pi) p_2(x)$, where $p_1(x)$ and $p_2(x)$ are normalized distributions with disjoint supports, and $\\pi \\in (0, 1)$.\n",
    "\n",
    "In the support of $p_1(x)$, $\\nabla_x \\log p_{data}(x) = \\nabla_x (\\log \\pi + \\log p_1(x)) = \\nabla_x \\log p_1(x)$, and in the support of $\\nabla_x \\log p_{data}(x) = \\nabla_x (\\log (1 - \\pi) + \\log p_2(x)) = \\nabla_x \\log p_2(x)$.\n",
    "\n",
    "In either case, the score $\\nabla_x \\log p_{data}(x)$ does not depend on $\\pi$.\n",
    "\n",
    "Since Langevin Dynamics use $\\nabla_x \\log p_{data}(x)$ to sample from $p_{data}(x)$, the samples obtained will not depend on $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, probs: List, mus: List[List], sigmas: List[List], device: str=\"cpu\") -> None:\n",
    "        assert sum(probs) == 1.\n",
    "        assert len(probs) == len(mus) == len(sigmas)\n",
    "        self.probs = torch.tensor(probs, device=device)\n",
    "        self.mus = torch.tensor(mus, device=device)\n",
    "        self.sigmas = torch.stack([\n",
    "            torch.diag(torch.tensor(sigma, device=device))\n",
    "            for sigma in sigmas\n",
    "        ])\n",
    "\n",
    "        self.gmm = TD.MixtureSameFamily(\n",
    "            TD.Categorical(self.probs),\n",
    "            TD.MultivariateNormal(self.mus, covariance_matrix=self.sigmas)\n",
    "        )\n",
    "    \n",
    "    def sample(self, shape: Tuple) -> Tensor:\n",
    "        return self.gmm.sample(shape)\n",
    "\n",
    "    def log_prob(self, samples: Tensor) -> Tensor:\n",
    "        return self.gmm.log_prob(samples)\n",
    "\n",
    "    def prob(self, samples: Tensor) -> Tensor:\n",
    "        return self.log_prob(samples).exp()\n",
    "    \n",
    "    def score(self, samples: Tensor) -> Tensor:\n",
    "        with torch.enable_grad():\n",
    "            samples = samples.detach()\n",
    "            samples.requires_grad_(True)\n",
    "            log_prob = self.log_prob(samples).sum()\n",
    "            return torch.autograd.grad(log_prob, samples)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangevinDynamics:\n",
    "    def __init__(self, score_fn: Callable, step_size: Tensor) -> None:\n",
    "        self.score_fn = score_fn\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def step(self, xt: Tensor, **kwargs: Dict) -> Tensor:\n",
    "        noise = torch.randn_like(xt)\n",
    "        return (\n",
    "            xt + \n",
    "            0.5 * self.step_size * self.score_fn(xt, **kwargs) +\n",
    "            self.step_size.sqrt() * noise\n",
    "        )\n",
    "\n",
    "    def __call__(self, xt: Tensor, T: int, **kwargs) -> Tensor:\n",
    "        for _ in range(T):\n",
    "            xt = self.step(xt, **kwargs)\n",
    "        return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [0.95, 0.05]\n",
    "mus = np.asarray([\n",
    "    [-1., -1.],\n",
    "    [1., 1.]\n",
    "]) * 5\n",
    "sigmas = np.asarray([\n",
    "    [1., 1.],\n",
    "    [1., 1.]\n",
    "])\n",
    "gmm = GMM(probs, mus, sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gmm.sample((1000, ))\n",
    "sns.scatterplot(x=samples[:, 0], y=samples[:, 1], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fn = lambda x: gmm.score(x)\n",
    "ld = LangevinDynamics(step_size=torch.tensor(0.01), score_fn=score_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "x_start = (torch.rand((1000, 2)) - 0.5) * 16.\n",
    "generations = ld(x_start, T)\n",
    "sns.scatterplot(x=generations[:, 0], y=generations[:, 1], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the range for x and y values for grid points\n",
    "x = np.linspace(-10, 10, 50)\n",
    "y = np.linspace(-10, 10, 50)\n",
    "\n",
    "# Create a meshgrid for x and y values\n",
    "X, Y = np.meshgrid(x, y)\n",
    "positions = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "positions_tensor = torch.tensor(positions, dtype=torch.float32)\n",
    "\n",
    "# Calculate the score for each grid point\n",
    "score = gmm.score(positions_tensor)\n",
    "\n",
    "# Convert score to numpy arrays for plotting\n",
    "U = score[:, 0].detach().numpy().reshape(X.shape)  # x-component of the score\n",
    "V = score[:, 1].detach().numpy().reshape(Y.shape)  # y-component of the score\n",
    "\n",
    "# Calculate the norm (magnitude) of the gradients\n",
    "norm = np.sqrt(U**2 + V**2)\n",
    "\n",
    "# Calculate the log_prob values\n",
    "log_prob = gmm.prob(positions_tensor).detach().numpy().reshape(X.shape)\n",
    "\n",
    "# Plot 1: Vector field of the score function\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.quiver(X, Y, U, V, color=\"blue\", alpha=0.75)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Score Function Vector Field of the GMM\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Contour plot of the norm of the gradients\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(X, Y, norm, levels=50, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Norm of the Gradient (Score Magnitude)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Norm of the Score Function Gradient\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Contour plot of the log-probability values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(X, Y, log_prob, levels=50, cmap=\"plasma\")\n",
    "plt.colorbar(label=\"Probability\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Probability Values of the GMM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: NCSN (Noise Conditioning Score Matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main idea** is perturbing data with Gaussian noise.\n",
    "\n",
    "1. Since the support of Gaussian noise distribution is the whole space, the perturbed data will not be confined to a low dimensional manifold, which obviates difficulties from the manifold hypothesis and makes score estimation well-defined.\n",
    "\n",
    "2. Large Gaussian noise has the effect of filling low density regions in the original unperturbed data distribution; therefore score matching may get more training signal to improve score estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\{\\sigma_i\\}_{i=1}^L$ be a geometric sequence, that satisfies\n",
    "$\\frac{\\sigma_{i - 1}}{\\sigma_i} = const > 1 \\ \\forall i$.\n",
    "\n",
    "Let $q_{\\sigma}(x) := \\int p_{data}(t) \\mathcal{N}(x | t, \\sigma^2 I)$.\n",
    "\n",
    "We choose $\\sigma_1$ large enough to mitigate aforementioned problems, and $\\sigma_L$ such $p_{data}(x) \\approx q_{\\sigma_{L}}(x)$\n",
    "\n",
    "We aim to estimate scores of all those distributions with a single model conditioned on a noise level, i.e. $s_{\\theta}(x, \\sigma) \\approx \\nabla_x \\log q_\\sigma(x)$.\n",
    "\n",
    "This model is called Noise Conditioning Score Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCSN objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every score function will be learned using Denoising Score Matching objective:\n",
    "$$\n",
    "\\begin{align}\n",
    "    l(\\theta, \\sigma) &=\n",
    "        \\dfrac{1}{2}\n",
    "        \\mathbb{E_{p_{data}(x)}}\n",
    "        \\mathbb{E_{x' \\sim \\mathcal{N}(x, \\sigma^2 I)}}\n",
    "        \\left\\|\n",
    "            s(\\theta, \\sigma)\n",
    "            -\n",
    "            \\nabla_{x'} \\log q(x' | x, \\sigma I)\n",
    "        \\right\\|^2\\\\\n",
    "        &=\n",
    "        \\dfrac{1}{2}\n",
    "        \\mathbb{E_{p_{data}(x)}}\n",
    "        \\mathbb{E_{x' \\sim \\mathcal{N}(x, \\sigma^2 I)}}\n",
    "        \\left\\|\n",
    "            s(\\theta, \\sigma)\n",
    "            +\n",
    "            \\dfrac{x' - x}{\\sigma^2}\n",
    "        \\right\\|^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the resulting objective has the following view:\n",
    "$$\n",
    "\\begin{align}\n",
    "    L(\\theta, \\{\\sigma_i\\}_{i=1}^L) =\n",
    "        \\dfrac{1}{L}\n",
    "        \\sum_{i = 1}^{L}\n",
    "        \\lambda(\\sigma_i)\n",
    "        l(\\theta, \\sigma_i).\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to choose $\\lambda(\\sigma)$?\n",
    "\n",
    "It was empirically shown, that trained model $\\|s_{\\theta}(x, \\sigma)\\| \\propto \\frac{1}{\\sigma}$.\n",
    "\n",
    "So, we can choose $\\lambda(\\sigma) = \\sigma^2$ in order to make every additive in loss independent of $\\sigma$:\n",
    "$$\n",
    "    \\lambda(\\sigma) l(\\theta, \\sigma)\n",
    "        =\n",
    "        \\dfrac{1}{2}\n",
    "        \\mathbb{E}\n",
    "        \\left\\|\n",
    "            \\sigma s(\\theta, \\sigma)\n",
    "            +\n",
    "            \\dfrac{x' - x}{\\sigma}\n",
    "        \\right\\|^2\\\\ \\ \\\\\n",
    "    \\|\\sigma s_{\\theta}(x, \\sigma)\\| \\propto 1\\\\ \\ \\\\\n",
    "    \\dfrac{x' - x}{\\sigma} \\sim \\mathcal{N}(0, I)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annealed Langevin Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pics/annealed_ld.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"pics/annealed_ld.png\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  NCSN on toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ncsn_utils import NCSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSCNTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ncsn: NCSN,\n",
    "        score_model: nn.Module,\n",
    "        train_iter: DataLoader,\n",
    "        lr: float,\n",
    "        weight_decay: float,\n",
    "        device: str=\"cpu\",\n",
    "        log_every: int=100,\n",
    "        plot_every: int=500\n",
    "    ) -> None:\n",
    "        self.ncsn = ncsn\n",
    "        self.score_model = score_model.to(device)\n",
    "        self.train_iter = train_iter\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.score_model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "        self.device = device\n",
    "        self.log_every = log_every\n",
    "        self.plot_every = plot_every\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "    def _train_step(self, x: Tensor) -> Tensor:\n",
    "        x = x.to(self.device)\n",
    "        loss = self.ncsn.train_loss(self.score_model, x)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, n_steps: int) -> None:\n",
    "        self.optimizer.zero_grad()\n",
    "        for step in range(n_steps):\n",
    "            x = next(self.train_iter)\n",
    "            batch_loss = self._train_step(x)\n",
    "\n",
    "            if (step + 1) % self.log_every == 0:\n",
    "                self.loss_history.append(batch_loss / x.size(0))\n",
    "            if (step + 1) % self.plot_every == 0:\n",
    "                self._plot_loss_history()\n",
    "\n",
    "    def _plot_loss_history(self) -> None:\n",
    "        clear_output()\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.show()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim: int, sigma_emb_dim: int, hidden_dim: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.sigma_emb = nn.Sequential(\n",
    "            nn.Linear(1, sigma_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(sigma_emb_dim, sigma_emb_dim),\n",
    "        )\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_dim + sigma_emb_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, in_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, sigma: Tensor) -> Tensor:\n",
    "        sigma_emb = self.sigma_emb(sigma)\n",
    "        x = torch.cat([x, sigma_emb], dim=1)\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Initialize an iterator over the dataset.\n",
    "        self.dataset_iterator = super().__iter__()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            batch = next(self.dataset_iterator)\n",
    "        except StopIteration:\n",
    "            # Dataset exhausted, use a new fresh iterator.\n",
    "            self.dataset_iterator = super().__iter__()\n",
    "            batch = next(self.dataset_iterator)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = InfiniteDataLoader(samples.to(torch.float32), batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "score_model = ScoreModel(2, 16, 128).to(device)\n",
    "ncsn = NCSN(\n",
    "    sigma_1=20,\n",
    "    sigma_L=0.1,\n",
    "    L=100,\n",
    "    base_step_size=0.001,\n",
    "    T=1000,\n",
    "    device=device\n",
    ")\n",
    "trainer = NSCNTrainer(\n",
    "    ncsn,\n",
    "    score_model,\n",
    "    train_loader,\n",
    "    lr=0.001,\n",
    "    weight_decay=0.0,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start = (torch.rand((5000, 2)) - 0.5) * 16.\n",
    "generations = ncsn.sample(score_model, x_start)\n",
    "sns.scatterplot(x=generations[:, 0], y=generations[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose hyperparameters?\n",
    "Details in the [paper](https://arxiv.org/abs/2006.09011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 0**:\n",
    "choose $\\sigma_L$ in a way that $q_{\\sigma_1}(x) \\approx \\pi(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 1** (Initial noise scale):\n",
    "choose $\\sigma_1$ to be as large as the maximum Euclidean distance between all pairs of training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 2** (Other noise scales):\n",
    "choose $\\{\\sigma_i\\}$ as a geometric prograssion with a common ratio $\\gamma$,\n",
    "such that $\\Phi(\\sqrt{2D}(\\gamma - 1) + 3 \\gamma) - \\Phi(\\sqrt{2D}(\\gamma - 1) - 3 \\gamma) \\approx 0.5$, where $D$ is a data dimensionality (should be large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technique 3** (selecting $T$ and $\\epsilon$):\n",
    "choose $T$ as large as allowed by a computing budget and then select an\n",
    "$\\epsilon$ that makes equation below maximally close to 1.\n",
    "\n",
    "$$\n",
    "    f(\\epsilon)\n",
    "    =\n",
    "    \\left(\n",
    "        1 - \\frac{\\epsilon}{\\sigma_L^2}\n",
    "    \\right)^{2T}\n",
    "    \\left(\n",
    "        \\gamma^2\n",
    "        -\n",
    "        \\frac{\n",
    "            2 \\epsilon\n",
    "        }{\n",
    "            \\sigma_L^2\n",
    "            -\n",
    "            \\sigma_L^2\n",
    "            (1 - \\frac{\\epsilon}{\\sigma_L^2})^2\n",
    "        }\n",
    "    \\right)\n",
    "    +\n",
    "    \\frac{\n",
    "        2 \\epsilon\n",
    "    }{\n",
    "        \\sigma_L^2\n",
    "        -\n",
    "        \\sigma_L^2\n",
    "        (1 - \\frac{\\epsilon}{\\sigma_L^2})^2\n",
    "    }\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
