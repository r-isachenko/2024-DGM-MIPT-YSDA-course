{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2d1386",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Generative Models</center>\n",
    "## <center>Seminar 12</center>\n",
    "\n",
    "<center>26.11.2024</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e8c1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan\n",
    "\n",
    "1. Recap\n",
    "    - U-Net\n",
    "    - Attention\n",
    "2. High-lever overwiev LDM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc7939",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## U-Net\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/1505.04597)\n",
    "\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20241126180422.png\" width=1300 /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960246e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## U-Net\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/1505.04597)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20241126154754.png\" width=1200 /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c86155",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention\n",
    "\n",
    "[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) is all you need.\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20241126181648.png\" width=1500 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0a8f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Diffusion Model\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2112.10752)\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20240511183056.png\" width=1300 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352463a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240511200618.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c5578",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240511200929.png\" width=1200 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e49f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LDM Recap\n",
    "\n",
    "**Encoder/Decoder:**\n",
    "- **Training**: Utilizes an autoencoder trained to compress images into a lower-dimensional latent space.\n",
    "- **Regularization**: Employs KL-divergence for slight penalty towards a normal distribution and a vector quantization (VQ) codebook to maintain discrete latent representations.\n",
    "\n",
    "**Diffusion Generator:** \n",
    "- Built on a U-Net structure integrated with **attention mechanisms** allow to use domain-specific encoder $\\tau_{\\theta}$ **conditioning on various modalities**: Semantic Map, Text Embeddings, Images, Binary Inpainting mask.\n",
    "\n",
    "\n",
    "**Multitask Capabilities:**\n",
    "- **Text-to-Image**: Transforms textual descriptions into corresponding high-resolution images using cross-attention to integrate text embeddings.\n",
    "- **Image Inpainting**: Fills in missing or removed parts of images, maintaining contextual coherence of the original image.\n",
    "- **Image-to-Image Translation**: Converts images from one domain to another (e.g., day to night scenes) leveraging the latent space embeddings.\n",
    "- **Super-Resolution**: Enhances the resolution of images from lower to higher resolutions, preserving image quality and detail.\n",
    "- **Unconditional Image Generation**: Generates images without specific input conditions, achieving diversity in output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c36d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Components of the pipeline\n",
    "\n",
    "1. **Encoder/Decoder** - VAE\n",
    "2. **Diffusion Generator** - U-Net with conditioning on various modalities: Text Embeddings, Images, Binary Inpainting mask, Semantic Map + TimeStamp\n",
    "3. **Text embedder** - CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ecf18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20240511183056.png\" width=1300 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2250a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Slides from [here](https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/stable-diffusion-scratch).\n",
    "\n",
    "<center><img src=\"attachments/Pasted image 20241126161943.png\" width=2000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a7afd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20241126162240.png\" width=900 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03e392",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20241126162001.png\" width=2000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d4d28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20241126162302.png\" width=2000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b833fba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20241126162346.png\" width=2000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef74fe73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20241126162421.png\" width=2000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22240bc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachments/Pasted image 20241126162526.png\" width=2000 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476285f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/r-isachenko/2024-DGM-MIPT-YSDA-course/blob/main/seminars/seminar12/seminar12_SD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
