{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyfGcNY4pcXd"
   },
   "source": [
    "# Homework 3: VQ-VAE and WGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvqXOhwQf0c2"
   },
   "source": [
    "## Task 1: Theory (3pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqU9vsu2CYLB"
   },
   "source": [
    "### Problem 1: ELBO surgery (1pt)\n",
    "\n",
    "At the lecture 5 we proved the [ELBO surgery](https://www.cs.columbia.edu/~blei/fogm/2020F/readings/HoffmanJohnson2016.pdf) theorem:\n",
    "$$\n",
    "    \\frac{1}{n} \\sum_{i=1}^n KL(q(\\mathbf{z} | \\mathbf{x}_i) || p(\\mathbf{z})) = KL(q_{\\text{agg}}(\\mathbf{z}) || p(\\mathbf{z})) + \\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}],\n",
    "$$\n",
    "where the first term is $KL(q_{\\text{agg}}(\\mathbf{z}) || p(\\mathbf{z}))$ includes the aggregated posterior distribution $q_{\\text{agg}}(\\mathbf{z})$ and the prior distribution $p(\\mathbf{z})$. Our goal now is to deal with the second term. At the lecture, the second term was equal to:\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}] = \\frac{1}{n}\\sum_{i=1}^n KL(q(\\mathbf{z} | \\mathbf{x}_i) || q_{\\text{agg}}(\\mathbf{z})).\n",
    "$$\n",
    "In fact, this is a mutual information between $\\mathbf{x}$ and $\\mathbf{z}$ on the empirical distribution of data and the distribution of $q(\\mathbf{z} | \\mathbf{x})$. Let treat the index of the sample $i$ as a random variable.\n",
    "$$\n",
    "    q(i, \\mathbf{z}) = q(i) q(\\mathbf{z} | i); \\quad p(i, \\mathbf{z}) = p(i) p(\\mathbf{z}); \\quad\n",
    "    q(i) = p(i) = \\frac{1}{n}.\n",
    "$$\n",
    "$$\n",
    "    \\quad q(\\mathbf{z} | i) = q(\\mathbf{z} | \\mathbf{x}_i) \\quad q_{\\text{agg}}(\\mathbf{z}) = \\sum_{i=1}^n q(i, \\mathbf{z}) = \\frac{1}{n} \\sum_{i=1}^n q(\\mathbf{z} | \\mathbf{x}_i);  \n",
    "$$\n",
    "Mutual information is a measure of independence between two random variables.\n",
    "$$\n",
    "\t\\mathbb{I}_{q} [\\mathbf{x}, \\mathbf{z}] = \\mathbb{E}_{q(i, \\mathbf{z})} \\log \\frac{q(i, \\mathbf{z})}{q(i)q_{\\text{agg}}(\\mathbf{z})}.\n",
    "$$\n",
    "Prove that 2 expressions for mutual information are equal to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1uu5MOXCaoj"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-E6G4FACeJM"
   },
   "source": [
    "### Problem 2: Gumbel-Max trick (1pt)\n",
    "\n",
    "In this problem you have to prove the Gumbel-Max trick that we have discussed at the Lecture 5.\n",
    "\n",
    "Let $\\pi_1, \\pi_2, \\dots \\pi_K, \\in (0, 1)$ and $\\sum\\limits_{k = 1}^{K} \\pi_k = 1$. Consider the discrete random variable:\n",
    "\n",
    "$$\n",
    "  c = \\arg\\max_{k} \\left[\\log \\pi_k + g_k\\right].\n",
    "$$\n",
    "\n",
    "In the formula above $g_k$ ($k \\in \\{1, \\dots K\\}$) are independent random variables distributed following the $\\text{Gumbel}(0, 1)$ distribution ([wiki](https://en.wikipedia.org/wiki/Gumbel_distribution)), i.e. $g_k \\sim \\text{Gumbel}(0, 1)$.\n",
    "\n",
    "Note that $g_k = - \\log (- \\log u)$, where $u \\sim \\text{Uniform}[0, 1]$.\n",
    "\n",
    "Our goal is to prove that $c \\sim \\text{Categorical}(\\pi_1, \\dots \\pi_K)$.\n",
    "\n",
    "1. Find cumulative distribution function ($F_{g}(x) = P(g < x)$) of Gumbel distribution.\n",
    "\n",
    "2. Find density of the Gumbel distribution (derivative of cdf).\n",
    "\n",
    "3. Consider random variables $\\zeta_k = \\log \\pi_k + g_k$. Let's fix $k^* \\in \\{1, \\dots K\\}$ and look at the following probability $P\\bigl( \\{\\zeta_{k} \\leq \\zeta_{k^*}\\} \\text{ for all } k \\neq k^*\\bigr)$. Prove that\n",
    "\n",
    "$$\n",
    "  P\\bigl( \\bigcap\\limits_{k \\neq k^*} \\{\\zeta_{k} \\leq \\zeta_{k^*}\\}\\bigr) = \\pi_{k^*}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO8IR1lKChKE"
   },
   "source": [
    "```your solution```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Least Squares GAN (1pt)\n",
    "    \n",
    "The Vanilla GAN often suffers from problems with a vanishing gradient. [Least Squares GAN](https://arxiv.org/abs/1611.04076) tries to solve this problem by replacing the error function with the following:\n",
    "$$\n",
    "   \t\\min_D V(D) = \\min_D \\frac{1}{2}\\left[ \\mathbb{E}_{\\pi(\\mathbf{x})} (D(\\mathbf{x}) - b)^2 + \\mathbb{E}_{p(\\mathbf{z})} (D(G(\\mathbf{z})) - a)^2 \\right]\n",
    "$$\n",
    "$$\n",
    "   \t\\min_G V(G) = \\min_G \\frac{1}{2}\\left[ \\mathbb{E}_{\\pi(\\mathbf{x})} (D(\\mathbf{x}) - c)^2 + \\mathbb{E}_{p(\\mathbf{z})} (D(G(\\mathbf{z})) - c)^2 \\right],\n",
    "$$\n",
    "where $a,b,c \\in \\mathbb{R}$ some fixed constants.\n",
    "\n",
    "1) Write out the formula for the optimal discriminator $D^*$.\n",
    "  \n",
    "2) Write out the expression for the error function of the generator $V(G)$ in the case of an optimal discriminator $D^*$.\n",
    "  \n",
    "3) Prove that for $b - c = 1$, $b - a = 2$, the error function of the generator $V(G)$ in the case of the optimal discriminator $D^*$ takes the form:\n",
    "$$\n",
    "   \tV(G) = \\chi^2_{\\text{Pearson}} \\left(\\frac{\\pi(\\mathbf{x}) + p(\\mathbf{x} | \\boldsymbol{\\theta})}{2} || p(\\mathbf{x} | \\boldsymbol{\\theta})\\right),\n",
    "$$\n",
    "where $\\chi^2_{\\text{Pearson}} (p || q)$ is a squared Pearson divergence:\n",
    "$$\n",
    "   \t\\chi^2_{\\text{Pearson}} (p || q) = \\int \\frac{(p(\\mathbf{x}) - q(\\mathbf{x}))^2}{p(\\mathbf{x})} d \\mathbf{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32394,
     "status": "ok",
     "timestamp": 1709365395802,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "9leRbWNBqZM2",
    "outputId": "1cc319b8-c194-4385-f28c-dfaacc7a4bb4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COMMIT_HASH = \"964c78c63a6b3b0781e5ef635c3138f5030e5e12\"\n",
    "!if [ -d dgm_utils ]; then rm -Rf dgm_utils; fi\n",
    "!git clone https://github.com/r-isachenko/dgm_utils.git\n",
    "%cd dgm_utils\n",
    "!git checkout {COMMIT_HASH}\n",
    "!pip install ./\n",
    "%cd ./..\n",
    "!rm -Rf dgm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8793,
     "status": "ok",
     "timestamp": 1709365404592,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "Nc5RAWFOqekr"
   },
   "outputs": [],
   "source": [
    "from dgm_utils import train_model, train_adversarial\n",
    "from dgm_utils import show_samples, visualize_images, load_dataset\n",
    "from dgm_utils import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1709365404592,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "EIBqEphlrEGd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print('GPU found :)')\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print('GPU not found :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3NaO2WkHdBo"
   },
   "source": [
    "## Task 2: VQ-VAE on MNIST (5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLFM5BtCHhDf"
   },
   "source": [
    "### Training of VQ-VAE model\n",
    "\n",
    "In this part you will train [VQ-VAE](https://arxiv.org/abs/1711.00937) model that we have discussed at the Lecture 5 (see also [VQ-VAE-2](https://arxiv.org/abs/1906.00446) paper).\n",
    "\n",
    "We will you MNIST dataset in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "executionInfo": {
     "elapsed": 1920,
     "status": "ok",
     "timestamp": 1709365695756,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "HmOaQgtGHfTm",
    "outputId": "cd84259c-7020-4f61-f76a-9960bd370dcf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "visualize_images(train_data, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJER2s8rHmXT"
   },
   "source": [
    "VQ-VAE model is a VAE model with discrete latent variable.  \n",
    "\n",
    "**Reminder:**\n",
    "* We define  dictionary (word book) space $\\{\\mathbf{e}_k\\}_{k=1}^K$, where $\\mathbf{e}_k \\in \\mathbb{R}^C$, $K$ is the size of the dictionary.\n",
    "* $\\mathbf{z}_e = \\text{NN}_e(\\mathbf{x}, \\boldsymbol{\\phi})$ - continuous output of encoder network.\n",
    "* $\\mathbf{z}_q = \\mathbf{e}_{k^*}$ is a quantized representation, where $k^* = \\text{argmin}_k \\| \\mathbf{z} - \\mathbf{e}_k \\|$. It is simple nearest neighbor look up.\n",
    "* Out deterministic variational posterior:\n",
    "$$\n",
    "  q(c = k^* | \\mathbf{x}, \\boldsymbol{\\phi}) = \\begin{cases}\n",
    "  1 , \\quad \\text{for } k^* = \\text{argmin}_k \\| \\mathbf{z}_e - \\mathbf{e}_k \\|; \\\\\n",
    "  0, \\quad \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "* Prior distribution is uniform: $p(c) = \\text{Uniform}\\{1, \\dots, K\\}$.\n",
    "* KL divergence between posterior and prior:\n",
    "$$\n",
    "  KL(q(c = k^* | \\mathbf{x}, \\boldsymbol{\\phi}), p(c)) = \\log K.\n",
    "$$\n",
    "* ELBO:\n",
    "$$\n",
    "\t\t\\mathcal{L} (\\boldsymbol{\\phi}, \\boldsymbol{\\theta})  = \\mathbb{E}_{q(c | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{e}_{c} , \\boldsymbol{\\theta}) - \\log K =  \\log p(\\mathbf{x} | \\mathbf{z}_q, \\boldsymbol{\\theta}) - \\log K.\n",
    "$$\n",
    "* Vector quantization is non-differentiable operation. We will use **straight-through** gradient estimator (we will copy gradients from decoder input $\\mathbf{z}_q$ to encoder output $\\mathbf{z}_e$.\n",
    "\n",
    "**Important modifications:**\n",
    "Due to the straight-through gradient estimation of mapping from $\\mathbf{z}_e$ to $\\mathbf{z}_q$, the embeddings $\\mathbf{e}$ receive no gradients from the ELBO.\n",
    "\n",
    "Therefore, in order to learn the embedding space we add l2 loss (**codebook loss**) to move the embedding vectors $\\mathbf{e}$ towards the encoder outputs $\\mathbf{z}_e$.\n",
    "\n",
    "Finally, since the volume of the embedding space is dimensionless, it can grow arbitrarily if the embeddings $\\mathbf{e}$ do not train as fast as the encoder parameters. To make sure the encoder commits to an embedding and its output does not grow, we add a **commitment loss**.\n",
    "\n",
    "Thus, the total training objective becomes:\n",
    "$$\n",
    "  \\log p(\\mathbf{x}| \\mathbf{z}_q, \\boldsymbol{\\theta}) + \\| \\text{stop\\_gradient}(\\mathbf{z}_e) - \\mathbf{e}\\|_2^2 + \\| \\mathbf{z}_e - \\text{stop\\_gradient}(\\mathbf{e})\\|_2.\n",
    "$$\n",
    "\n",
    "Pay attention to the $\\text{stop\\_gradient}(*)$ operator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2o9DFErHqbp"
   },
   "source": [
    "Our first step is implement vector quantization procedure. It will also calculate two consistency losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1709365696135,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "crnRkktfHm2v"
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings: int = 128, embedding_dim: int = 16, beta: float = 0.25\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "    def get_code_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = x.shape[:-1]\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) calculate distances from flatten inputs to embeddings\n",
    "        # 2) find nearest embeddings to each input (use argmin op)\n",
    "\n",
    "        # ====\n",
    "        encoding_indices = encoding_indices.view(input_shape)\n",
    "        return encoding_indices\n",
    "\n",
    "    def get_quantized(self, encoding_indices: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get embeddgins with appropriate indices\n",
    "        # 2) transform tensor from BHWC to BCHW format\n",
    "\n",
    "        # ====\n",
    "        return quantized\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) get indices\n",
    "        # 2) get quantized latents\n",
    "        # 3) calculate codebook and commitment loss\n",
    "        #    do not afraid about stop_gradient op\n",
    "        #    (use .detach() method for quantized latents and x)\n",
    "        # 4) final loss is codebook_loss + beta * commitment_loss\n",
    "        \n",
    "        # ====\n",
    "\n",
    "        # Straight-through estimator (think about it!).\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        return quantized, loss\n",
    "\n",
    "\n",
    "def test_vector_quantizer():\n",
    "    x = torch.zeros((1, 16, 7, 7))\n",
    "    layer = VectorQuantizer()\n",
    "    indices = layer.get_code_indices(x)\n",
    "    assert indices.shape == (1, 7, 7)\n",
    "    quantized = layer.get_quantized(indices)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    quantized, loss = layer(x)\n",
    "    assert quantized.shape == (1, 16, 7, 7)\n",
    "    assert loss.shape == ()\n",
    "\n",
    "\n",
    "test_vector_quantizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R2KmkLJHt2S"
   },
   "source": [
    "We will use simple encoder/decoder with several strided convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1709365696136,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "wgin3a_nHvsM"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code\n",
    "        # define Sequential model with Conv2d and ReLU activation\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # ====\n",
    "        # your code\n",
    "        # define Sequential model with ConvTransposed2d and ReLU activation\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxTB_hRjHxsb"
   },
   "source": [
    "Now we are ready to define our model. It consists of encoder, decoder and vector quantizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1101,
     "status": "ok",
     "timestamp": 1709365697235,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "G6Tam1vLHz8q"
   },
   "outputs": [],
   "source": [
    "class VQVAEModel(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ce_loss_scale: float = 1.0,\n",
    "        latent_dim: int = 16,\n",
    "        num_embeddings: int = 64,\n",
    "        latent_size: tuple = (7, 7),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(latent_dim)\n",
    "        self.decoder = ConvDecoder(latent_dim)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, latent_dim)\n",
    "        self.ce_loss_scale = ce_loss_scale\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder\n",
    "        # 2) apply vector quantizer (it returns quantized representation + vq_loss)\n",
    "        # 3) apply decoder (it returns decoded samples)\n",
    "\n",
    "        # ====\n",
    "        return decoded, vq_loss\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply model\n",
    "        # 2) get cross entropy loss\n",
    "\n",
    "        # ====\n",
    "        return {\n",
    "            \"total_loss\": self.ce_loss_scale * ce_loss + vq_loss,\n",
    "            \"ce_loss\": self.ce_loss_scale * ce_loss,\n",
    "            \"vq_loss\": vq_loss,\n",
    "        }\n",
    "\n",
    "    def get_indices(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder\n",
    "        # 2) get indices of codes using vector quantizer\n",
    "\n",
    "        # ====\n",
    "        return codebook_indices\n",
    "\n",
    "    def prior(self, n: int) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # prior distribution is uniform\n",
    "        # 1) get samples from categorical distribution\n",
    "        # 2) get quantized representations using vector quantizer\n",
    "\n",
    "        # ====\n",
    "        return quantized\n",
    "\n",
    "    def sample_from_logits(self, logits: torch.Tensor) -> np.ndarray:\n",
    "        # ====\n",
    "        # your code\n",
    "        # our model will return logits, this method applies softmax and samples from the distribution\n",
    "        # 1) apply softmax to the logits\n",
    "        # 2) sample from the distribution (e.x. you could use torch.multinomial)\n",
    "        # be careful with the sizes of the tensors (may be you need to permute/reshape dimensios)\n",
    "\n",
    "        # ====\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) sample from prior distribution\n",
    "            # 2) apply decoder\n",
    "            # 3) sample from logits\n",
    "\n",
    "            # ====\n",
    "            return samples\n",
    "\n",
    "\n",
    "def test_vqvae_model():\n",
    "    model = VQVAEModel().cuda()\n",
    "    x = torch.zeros((2, 1, 28, 28)).cuda()\n",
    "\n",
    "    encoded = model.encoder(x)\n",
    "    size = encoded.shape[2:]\n",
    "    assert size == model.latent_size\n",
    "\n",
    "    indices = model.get_indices(x)\n",
    "    assert indices.shape == (2, 7, 7)\n",
    "\n",
    "    losses = model.loss(x)\n",
    "    assert isinstance(losses, dict)\n",
    "    assert \"total_loss\" in losses\n",
    "\n",
    "    quantized = model.prior(10)\n",
    "    assert quantized.shape == (10, 16, *model.latent_size)\n",
    "\n",
    "    decoded = model.decoder(quantized)\n",
    "    assert decoded.shape == (10, 2, 28, 28)\n",
    "\n",
    "    sampled = model.sample(10)\n",
    "    assert sampled.shape == (10, 1, 28, 28)\n",
    "\n",
    "\n",
    "test_vqvae_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MTgyRyQH14x"
   },
   "source": [
    "Let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490,
     "referenced_widgets": [
      "03dc614f4b9943ab91923025f5c5b0e0",
      "ebfed28360ad4dffab39a9d23ec8bbfc",
      "c21d4335872e463590799841ec35b1eb",
      "db1ebda23ce8424a857d3e73846fc0df",
      "1a14cbb10b9e4942baff7b389ac0c299",
      "a2d8ebaf188b481ba242ff07990b581d",
      "83fdc33f980d41c4a600531fbeab2fc0",
      "72a8540e9ce24b0f9c38a4a29c1377de",
      "303ef90204bd48e3a5423fac513cbb49",
      "c21c86b35c1c4b178d23f884add233bb",
      "cca2526aa9dc49eebd3e55e2f553e89b"
     ]
    },
    "executionInfo": {
     "elapsed": 40810,
     "status": "ok",
     "timestamp": 1709365738043,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "5pssXU7ZH3S7",
    "outputId": "b4234f9a-f357-429e-ee83-34ae7cb7810d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE =\n",
    "EPOCHS =\n",
    "LR =\n",
    "CE_SCALE =\n",
    "# ====\n",
    "\n",
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "\n",
    "model = VQVAEModel(ce_loss_scale=CE_SCALE, latent_dim=16, num_embeddings=128)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# choose any optimizer/scheduler as you want\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# ====\n",
    "\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    n_samples=16,\n",
    "    visualize_samples=True,\n",
    "    logscale_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TvS_fGKH5Is"
   },
   "source": [
    "Now we is able to sample from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1709365738852,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "ebHjZdfyH6es",
    "outputId": "f3b7555b-c3dd-43da-b959-8e9c4f599f77"
   },
   "outputs": [],
   "source": [
    "# Samples\n",
    "samples = model.sample(100)\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")\n",
    "\n",
    "# Reconstructions\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    decoded, _ = model(x)\n",
    "    x_recon = model.sample_from_logits(decoded)\n",
    "x = x.cpu().numpy()\n",
    "reconstructions = np.concatenate((x, x_recon), axis=0)\n",
    "reconstructions = reconstructions.astype(\"float32\")\n",
    "show_samples(reconstructions, title=\"Reconstructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRtQoLfWH8h_"
   },
   "source": [
    "Probably you will get bad samples :(\n",
    "\n",
    "Do not worry, may be it is ok, we will try to fix your samples! Make sure that reconstructions are almost perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dg61P0IH-de"
   },
   "source": [
    "Here, we will visualize latent code indices for test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1709365739458,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "IFbRkzkpH_y-",
    "outputId": "ba87e656-a01d-45b2-d647-7f8491f3e905"
   },
   "outputs": [],
   "source": [
    "test_images = next(iter(test_loader))[:100]\n",
    "x = test_images.cuda()\n",
    "codebook_indices = model.get_indices(x).cpu().unsqueeze(1)\n",
    "\n",
    "show_samples(test_images, \"Test images\")\n",
    "show_samples(codebook_indices, \"Test codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn2OJFvEIBb_"
   },
   "source": [
    "### Training of prior autoregressive model\n",
    "\n",
    "The samples from our VQ-VAE model is not good enough. The authors of the original VQ-VAE paper proposed to train autoregressive model in the latent space after we trained VQ-VAE model.\n",
    "\n",
    "Remember we have discussed **ELBO surgery** and **aggregrated posterior**. Let recall what do we have in VAE:\n",
    "* **Training:** we get latent variables $\\mathbf{z}$ from variational posterior $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})$ for every object $\\mathbf{x}$ and then applies decoder ($p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})$). It means that in average decoder is applied to the latent variables from aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$.\n",
    "* **Inference:** We apply decoder to the latent variables from prior distribution $p(\\mathbf{z})$.\n",
    "\n",
    "It means that if our aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$ and prior $p(\\mathbf{z})$ is too far from each other, then we get inconsistency.\n",
    "\n",
    "So let train to remove this inconsistency. To be concrete, let train (autoregressive) model in the latent space that will try to predict samples from the aggregated posterior $q_{\\text{agg}}(\\mathbf{z} | \\boldsymbol{\\phi})$.\n",
    "\n",
    "We will use our good friend: PixelCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1709365739459,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "hx4pWRNhIDTE"
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self, mask_type: str, in_channels: int, out_channels: int, kernel_size: int = 5\n",
    "    ) -> None:\n",
    "        assert mask_type in [\"A\", \"B\"]\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.register_buffer(\"mask\", torch.zeros_like(self.weight))\n",
    "        self.create_mask(mask_type)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def create_mask(self, mask_type: str) -> None:\n",
    "        # ====\n",
    "        # your code\n",
    "        # do not forget about mask_type\n",
    "\n",
    "        # ====\n",
    "\n",
    "\n",
    "def test_masked_conv2d():\n",
    "    layer = MaskedConv2d(\"A\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.zeros((2, 2)))\n",
    "\n",
    "    layer = MaskedConv2d(\"B\", 2, 2)\n",
    "    assert np.allclose(layer.mask[:, :, 2, 2].numpy(), np.ones((2, 2)))\n",
    "\n",
    "\n",
    "test_masked_conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1709365739459,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "gQXIckZpIEyY"
   },
   "outputs": [],
   "source": [
    "class PixelCNN(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int = 128,\n",
    "        input_shape: tuple = (7, 7),\n",
    "        n_filters: int = 32,\n",
    "        kernel_size: int = 5,\n",
    "        n_layers: int = 5,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply the sequence of MaskedConv2d -> ReLU\n",
    "        # the last layer should be MaskedConv2d (not ReLU)\n",
    "        # Note 1: the first conv layer should be of type 'A'\n",
    "        # Note 2: final output_dim in MaskedConv2d must be 2\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # read the forward method carefully\n",
    "        flattened = x.view((-1, 1))\n",
    "        encodings = torch.zeros(flattened.shape[0], self.num_embeddings).cuda()\n",
    "        encodings.scatter_(1, flattened, 1)\n",
    "        encodings = encodings.view((-1, *self.input_shape, self.num_embeddings))\n",
    "        encodings = encodings.permute((0, 3, 1, 2))\n",
    "        out = self.net(encodings)\n",
    "        out = out.view(-1, self.num_embeddings, 1, *self.input_shape)\n",
    "        return out\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "        \n",
    "        # ====\n",
    "        return {\"total_loss\": total_loss}\n",
    "\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        # read carefully the sampling process\n",
    "        samples = torch.zeros(n, 1, *self.input_shape, dtype=torch.int64).cuda()\n",
    "        with torch.no_grad():\n",
    "            for r in range(self.input_shape[0]):\n",
    "                for c in range(self.input_shape[1]):\n",
    "                    logits = self(samples)[:, :, :, r, c]\n",
    "                    probs = F.softmax(logits, dim=1).squeeze(-1)\n",
    "                    samples[:, 0, r, c] = torch.multinomial(\n",
    "                        probs, num_samples=1\n",
    "                    ).squeeze(-1)\n",
    "        return samples.cpu().numpy()\n",
    "\n",
    "\n",
    "def test_pixelcnn():\n",
    "    model = PixelCNN().cuda()\n",
    "    x = torch.zeros((1, 1, 7, 7), dtype=torch.int64).cuda()\n",
    "    output = model(x)\n",
    "    assert output.shape == (1, 128, 1, 7, 7)\n",
    "    losses = model.loss(x)\n",
    "    assert isinstance(losses, dict)\n",
    "    assert \"total_loss\" in losses\n",
    "    samples = model.sample(10)\n",
    "    assert samples.shape == (10, 1, 7, 7)\n",
    "\n",
    "\n",
    "test_pixelcnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU2O_ZUSIGYn"
   },
   "source": [
    "Now we need to get our train and test samples. Our model will predict indices of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1709365739459,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "pzCCgD0dIHwz"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# you have to get indices of the emdeddings from the VQ-VAE model for train and test data\n",
    "INPUT_SHAPE =  # input shape of your latent space\n",
    "\n",
    "# ====\n",
    "\n",
    "assert isinstance(train_indices, np.ndarray)\n",
    "assert isinstance(test_indices, np.ndarray)\n",
    "assert train_indices.shape == (60000, 1, *INPUT_SHAPE)\n",
    "assert test_indices.shape == (10000, 1, *INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6edd5962dc354834993e41c0d54c94de",
      "49d624d6489f44dda136bf79960314fc",
      "b4fdcf9c7cab40319e09a91ff1a32e75",
      "5c7042457920424384aeb4656a447ef5",
      "364720107e0041bc8cc60148675a0ae9",
      "3fe0bf255eba43859f9d207646f1255c",
      "0b290cfa6bfc4cd4844200d2b088a528",
      "81a7a92d5f6a4e15a332226aab686610",
      "2feb36240afe434495d2bc45053d1103",
      "d1990dda9bd24d6692447b4258809c98",
      "6c630a6f97404188aaf853956b81d34c"
     ]
    },
    "executionInfo": {
     "elapsed": 21738,
     "status": "ok",
     "timestamp": 1709365761189,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "bRDOTXifIJHJ",
    "outputId": "c88316ec-37c1-4730-c554-1c8225e75ae6"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters by your own\n",
    "EPOCHS =\n",
    "BATCH_SIZE =\n",
    "LR =\n",
    "N_LAYERS =\n",
    "N_FILTERS =\n",
    "# ====\n",
    "\n",
    "prior_model = PixelCNN(\n",
    "    input_shape=INPUT_SHAPE, n_filters=N_FILTERS, kernel_size=5, n_layers=N_LAYERS\n",
    ")\n",
    "\n",
    "train_loader = data.DataLoader(train_indices, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_indices, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# choose any optimizer/scheduler as you want\n",
    "optimizer = torch.optim.Adam(prior_model.parameters(), lr=LR)\n",
    "# ====\n",
    "\n",
    "train_model(\n",
    "    prior_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    visualize_samples=False,\n",
    "    logscale_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfgh_cRXIKio"
   },
   "source": [
    "Now we are ready to sample from our VQ-VAE model. The difference here that we will sample our embedding indices from the PixelCNN prior model instead of the Uniform prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1709365761851,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "02161952814365046944"
     },
     "user_tz": -180
    },
    "id": "CpAPrirKIL57",
    "outputId": "bfd1a9cc-5c01-4b11-88bf-41184b973002",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 100\n",
    "indices = prior_model.sample(N_SAMPLES).squeeze(1)\n",
    "quantized = model.vq_layer.get_quantized(torch.Tensor(indices).int().cuda())\n",
    "logits = model.decoder(quantized)\n",
    "samples = model.sample_from_logits(logits)\n",
    "\n",
    "samples = samples.astype(\"float32\")\n",
    "show_samples(samples, title=\"Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8klUHHmAjHsI"
   },
   "source": [
    "## Task 3: Wasserstein GANs for CIFAR 10 (5pt)\n",
    "\n",
    "In this task you will fit different kinds of Wasserstein GANs (different ways to enforce Lipschitzness) that we discussed at the Lecture 6 to the CIFAR10 dataset\n",
    "* [WGAN](https://arxiv.org/abs/1701.07875) - standard Wasserstein GAN with weight clipping;\n",
    "* [WGAN-GP](https://arxiv.org/pdf/1704.00028.pdf) - Wasserstein GAN with Gradient Penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "dAvSLXlKZ5q7",
    "outputId": "7b7c65bd-d4fa-437b-e99e-f5a7ae35dfd8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"cifar10\", flatten=False, binarize=False)\n",
    "visualize_images(train_data, \"CIFAR10 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PnfAHgw32Hr"
   },
   "source": [
    "### Problem 1: WGAN (3pt)\n",
    "\n",
    "[WGAN](https://arxiv.org/abs/1701.07875) model uses weight clipping to enforce Lipschitzness of the critic.\n",
    "\n",
    "The model objective is\n",
    "$$\n",
    "\\min_{G} W(\\pi || p) \\approx \\min_{G} \\max_{\\boldsymbol{\\phi} \\in \\boldsymbol{\\Phi}} \\left[ \\mathbb{E}_{\\pi(\\mathbf{x})} f_{\\boldsymbol{\\phi}}(\\mathbf{x})  - \\mathbb{E}_{p(\\mathbf{z})} f_{\\boldsymbol{\\phi}}(G_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\right].\n",
    "$$\n",
    "Here $f_{\\boldsymbol{\\phi}}(\\mathbf{x})$ is the critic model. The critic weights $\\boldsymbol{\\phi}$ should lie in the compact set $\\boldsymbol{\\Phi} = [-c, c]^d$.\n",
    "\n",
    "In this task we will use convolutional networks for the generator $G_{\\boldsymbol{\\theta}}(\\mathbf{z})$ and the critic $f_{\\boldsymbol{\\phi}}(\\mathbf{x})$.\n",
    "\n",
    "First of all, let define generator network. It will be the same for all WGAN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "7a7fjcx0vL8R"
   },
   "outputs": [],
   "source": [
    "class ConvGenerator(nn.Module):\n",
    "    def __init__(self, input_size: int = 128, n_channels: int = 64) -> None:\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.input_size = input_size\n",
    "        # ====\n",
    "        # your code\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply all layers\n",
    "        \n",
    "        # ====\n",
    "        return output.view(-1, 3, 32, 32)\n",
    "        \n",
    "        \n",
    "def test_conv_generator():\n",
    "    model = ConvGenerator(input_size=4, n_channels=32)\n",
    "    x = torch.randn((2, 4))\n",
    "    out = model(x)\n",
    "    assert list(out.size()) == [2, 3, 32, 32], out.size()\n",
    "    \n",
    "test_conv_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uVbwHvAruFh"
   },
   "source": [
    "Now it is time to define our critic. Here we will use the same class for all WGAN models, but the arguments will depend on the WGAN mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "YFA0tI7ZrloP"
   },
   "outputs": [],
   "source": [
    "class ConvCritic(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_channels: int, clip_c: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.clip_c = clip_c\n",
    "        \n",
    "        # ====\n",
    "        # your code\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def clip_weights(self) -> None:\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear) or isinstance(layer, nn.Conv2d):\n",
    "                # ====\n",
    "                # your code\n",
    "                # clip the weight to the range [-clip_c, clip_c]\n",
    "\n",
    "                # ====\n",
    "                layer.weight.data = weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) clip the critic weights (if clip_c is given)\n",
    "        # 2) apply all layers\n",
    "\n",
    "        # ====\n",
    "        return output\n",
    "        \n",
    "        \n",
    "def test_conv_critic():\n",
    "    model = ConvCritic(n_channels=4, clip_c=0.01)\n",
    "    x = torch.randn((2, 3, 32, 32))\n",
    "    out = model(x)\n",
    "    assert list(out.size()) == [2, 1], out.size()\n",
    "\n",
    "    \n",
    "test_conv_critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 128, \n",
    "        n_channels: int = 64, \n",
    "        clip_c: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.generator = ConvGenerator(input_size, n_channels)\n",
    "        self.discriminator = ConvCritic(n_channels, clip_c)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.generator(x)\n",
    "    \n",
    "    def loss_generator(self, batch_size) -> dict:\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) sample batch \n",
    "        # NOTE: sample function uses torch.no_grad\n",
    "        # 2) apply generator loss (only term with genrator)\n",
    "\n",
    "        # ====\n",
    "        return {\"generator_loss\": g_loss}\n",
    "\n",
    "    def loss_discriminator(self, x: torch.Tensor) -> dict:\n",
    "        fake_data = self.sample(x.shape[0])\n",
    "        # ====\n",
    "        # your code\n",
    "        # calculate critic loss (both terms)\n",
    "\n",
    "        # ====\n",
    "        return {\"critic_loss\": d_loss, \"critic_loss_fake\": d_loss_fake, \"critic_loss_real\": d_loss_real}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples: int) -> torch.Tensor:\n",
    "        # ====\n",
    "        # your code\n",
    "        # sample from standard normal distribution and apply the model\n",
    "\n",
    "        # ====\n",
    "        return output\n",
    "    \n",
    "def test_wgan():\n",
    "    model = WGAN(input_size=4, n_channels=32)\n",
    "    x = torch.randn((2, 4))\n",
    "    out = model(x)\n",
    "    assert list(out.size()) == [2, 3, 32, 32], out.size()\n",
    "    \n",
    "    out = model.sample(10)\n",
    "    assert list(out.size()) == [10, 3, 32, 32], out.size()\n",
    "\n",
    "    loss = model.loss_generator(10)\n",
    "    assert isinstance(loss, dict)\n",
    "    \n",
    "    x = torch.randn((2, 3, 32, 32))\n",
    "    loss = model.loss_discriminator(x)\n",
    "    assert isinstance(loss, dict)\n",
    "    \n",
    "test_wgan()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2d52084a512f4055ad01a9afbfaffff5",
      "1f880ad04f534e08aad0a006a33ed646",
      "25a3dc242d10469abc5a211dbc0746ca",
      "a9b2b099d36149dda1a92bb1eece775b",
      "d3b97e8914794f98bf81d84a2aadfe52",
      "8fca1a010dc84a2aadfc6b4e5f15d1f6",
      "d013d0fa6ee74b4ead7dc4f66d55f77c",
      "fb1bf34d7e344a5f9f459c0905548652",
      "144b17bde62042ac86f4c3507240ac1e",
      "abc7b4bf2975473e8de753c032e6e212",
      "7d18d94ae32941f7adfab36d7b49f61e"
     ]
    },
    "id": "bjOGNNU59Fr_",
    "outputId": "693ddaeb-0172-4dbd-e559-2757ecd5dc53"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE =\n",
    "N_CHANNELS =\n",
    "EPOCHS = \n",
    "CRITIC_STEPS = \n",
    "CLIP_C = \n",
    "LR = \n",
    "# ====\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "wgan = WGAN(input_size=128, n_channels=N_CHANNELS, clip_c=CLIP_C)\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# choose any optimizer as you want\n",
    "generator_optimizer = torch.optim.Adam(wgan.generator.parameters(), lr=LR, betas=(0, 0.9))\n",
    "discriminator_optimizer = torch.optim.Adam(wgan.discriminator.parameters(), lr=LR, betas=(0, 0.9))\n",
    "# ====\n",
    "\n",
    "train_adversarial(\n",
    "    wgan,\n",
    "    train_loader,\n",
    "    epochs=EPOCHS,\n",
    "    d_steps=CRITIC_STEPS,\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    device=DEVICE,\n",
    "    generator_loss_key = \"generator_loss\",\n",
    "    discriminator_loss_key = \"critic_loss\",\n",
    "    n_samples=100,\n",
    "    visualize_samples=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2oddhso98hq"
   },
   "source": [
    "Let sample from our model and draw the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "EXWKEKxQ9_DH",
    "outputId": "cc8bff11-01f1-40f4-dd9b-470b1f3a1b94"
   },
   "outputs": [],
   "source": [
    "wgan.eval()\n",
    "samples = wgan.sample(100).cpu().detach().numpy()\n",
    "\n",
    "show_samples(samples, title=\"CIFAR-10 WGAN-generated samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG4ErQLNjZr9"
   },
   "source": [
    "### Problem 2: WGAN-GP for CIFAR 10 (2pt)\n",
    "\n",
    "Another way to enforce Lipschitzness comes from the following [theorem](https://arxiv.org/pdf/1704.00028.pdf):\n",
    "\n",
    "Let $\\pi(\\mathbf{x})$ and $p(\\mathbf{x})$ be two distribution in $\\mathcal{X}$, a compact metric space. Let $\\gamma$ be the optimal transportation plan between $\\pi(\\mathbf{x})$ and $p(\\mathbf{x})$. Then, there is 1-Lipschitz function $f^*$ which is the optimal solution of \n",
    "$$\n",
    "    \\max_{\\| f \\|_L \\leq 1} \\left[ \\mathbb{E}_{\\pi(\\mathbf{x})} f(\\mathbf{x})  - \\mathbb{E}_{p(\\mathbf{x})} f(\\mathbf{x})\\right].\n",
    "$$\n",
    "\n",
    "if $f^*$ is differentiable, $\\gamma(\\mathbf{y} = \\mathbf{z}) = 0$ and $\\hat{\\mathbf{x}}_t = t \\mathbf{y} + (1 - t) \\mathbf{z}$ with $\\mathbf{y} \\sim \\pi(\\mathbf{x})$, $\\mathbf{z} \\sim p(\\mathbf{x} | \\mathbf{\\theta})$, $t \\in [0, 1]$ it holds that\n",
    "$$\n",
    "    \\mathbb{P}_{(\\mathbf{y}, \\mathbf{z}) \\sim \\gamma} \\left[ \\nabla f^*(\\hat{\\mathbf{x}}_t) = \\frac{\\mathbf{z} - \\hat{\\mathbf{x}}_t}{\\| \\mathbf{z} - \\hat{\\mathbf{x}}_t \\|} \\right] = 1.\n",
    "$$\n",
    "\n",
    "This follows that $f^*$ has gradient norm 1 almost everywhere under $\\pi(\\mathbf{x})$ and $p(\\mathbf{x})$. Thus, we can add a term that will enforce the gradient to equal 1:\n",
    "$$\n",
    " W(\\pi || p) = \\underbrace{\\mathbb{E}_{\\pi(\\mathbf{x})} f(\\mathbf{x})  - \\mathbb{E}_{p(\\mathbf{x} | \\boldsymbol{\\theta})} f(\\mathbf{x})}_{\\text{original critic loss}} + \\lambda \\underbrace{\\mathbb{E}_{U[0, 1]} \\left[ \\left( \\| \\nabla_{\\hat{\\mathbf{x}}} f(\\hat{\\mathbf{x}}) \\|_2 - 1 \\right) ^ 2\\right]}_{\\text{gradient penalty}},\n",
    "$$\n",
    "where $t \\sim U[0,1]$ is uniformly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aTCJKgoAuUm"
   },
   "source": [
    "Let define our gradient penalty loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "9RXJ6autArvg"
   },
   "outputs": [],
   "source": [
    "class WGAN_GP(WGAN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gp_weight: float,\n",
    "        input_size: int = 128, \n",
    "        n_channels: int = 64, \n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.gp_weight = gp_weight\n",
    "        self.generator = ConvGenerator(input_size, n_channels)\n",
    "        self.discriminator = ConvCritic(n_channels, None)\n",
    "\n",
    "    def gradient_penalty(\n",
    "        self, real_data: torch.Tensor, fake_data: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size = real_data.shape[0]\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # Calculate interpolation x_t = t * x_real + (1 - t) x_fake\n",
    "        # 1) sample t\n",
    "        # 2) create x_t (be careful about shapes)\n",
    "        # 3) apply critic to x_t\n",
    "        \n",
    "        # ====\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_output,\n",
    "            inputs=x_t,\n",
    "            grad_outputs=torch.ones(d_output.size()).to(fake_data.device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        gradients = gradients.reshape(batch_size, -1)\n",
    "        # ====\n",
    "        # your code\n",
    "        # compute gradient norm\n",
    "        \n",
    "        # ====\n",
    "        return ((gradients_norm - 1) ** 2).mean()\n",
    "    \n",
    "    def loss_discriminator(self, x: torch.Tensor) -> dict:\n",
    "        fake_data = self.sample(x.shape[0])\n",
    "        # ====\n",
    "        # your code\n",
    "        # calculate critic loss (both terms)\n",
    "        \n",
    "        # ====\n",
    "        losses = {\n",
    "            \"critic_loss\": d_loss, \n",
    "            \"critic_loss_fake\": d_loss_fake, \n",
    "            \"critic_loss_real\": d_loss_real,\n",
    "            \"gradient_penalty_loss\": gp\n",
    "        }\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBRZJXGAryNA"
   },
   "source": [
    "That is all :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432,
     "referenced_widgets": [
      "7d0563bf68604b8b9614d4fe0a8f992b",
      "950787f7d6934287a86a0a38c57ae29d",
      "89eb7c59a8e84fefa0db7f440f6848bb",
      "8afb4f28944145c3b90bacb6cb3b6327",
      "77b9f77796ab4344a23847d02e2087e0",
      "f29ce587aa53499c9c9e9c589be8f320",
      "518e7454eaa14f10ada3781a2401b622",
      "0057c56e43504274858faf340424348c",
      "639c32ab53164d7f9df57757b6e5c9c2",
      "a25e239d5cfd4c04a0569ee909df63e5",
      "bf5f3024f475431ca24c96f147255d75"
     ]
    },
    "id": "aY6qzJ5MSuiX",
    "outputId": "15b8c85e-24eb-4db3-8282-6736bcab5ddb"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "N_CHANNELS = \n",
    "EPOCHS = \n",
    "CRITIC_STEPS = \n",
    "GP_WEIGHT = \n",
    "LR = \n",
    "# ====\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "wgan_gp = WGAN_GP(gp_weight=GP_WEIGHT, input_size=128, n_channels=N_CHANNELS)\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# choose any optimizer as you want\n",
    "generator_optimizer = torch.optim.Adam(wgan_gp.generator.parameters(), lr=LR, betas=(0, 0.9))\n",
    "discriminator_optimizer = torch.optim.Adam(wgan_gp.discriminator.parameters(), lr=LR, betas=(0, 0.9))\n",
    "# ====\n",
    "\n",
    "train_adversarial(\n",
    "    wgan_gp,\n",
    "    train_loader,\n",
    "    epochs=EPOCHS,\n",
    "    d_steps=CRITIC_STEPS,\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    device=DEVICE,\n",
    "    generator_loss_key = \"generator_loss\",\n",
    "    discriminator_loss_key = \"critic_loss\",\n",
    "    n_samples=100,\n",
    "    visualize_samples=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unAqs_pEs59l"
   },
   "source": [
    "Let sample from our model and draw the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8I0pNzHchqRs"
   },
   "outputs": [],
   "source": [
    "wgan_gp.eval()\n",
    "samples = wgan.sample(100).cpu().detach().numpy()\n",
    "\n",
    "show_samples(samples, title=\"CIFAR-10 WGAN-GP-generated samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmPH_hO0INa4"
   },
   "source": [
    "Here you have to get samples with good enough quality!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03dc614f4b9943ab91923025f5c5b0e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebfed28360ad4dffab39a9d23ec8bbfc",
       "IPY_MODEL_c21d4335872e463590799841ec35b1eb",
       "IPY_MODEL_db1ebda23ce8424a857d3e73846fc0df"
      ],
      "layout": "IPY_MODEL_1a14cbb10b9e4942baff7b389ac0c299"
     }
    },
    "0b290cfa6bfc4cd4844200d2b088a528": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "159726388b174e5caa99596a7dba1757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9928da49da140ee9f7c647d9c4dd25b",
      "placeholder": "​",
      "style": "IPY_MODEL_e139d57637af473e9254cdbb5da91fc2",
      "value": "100%"
     }
    },
    "1a14cbb10b9e4942baff7b389ac0c299": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2feb36240afe434495d2bc45053d1103": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "303ef90204bd48e3a5423fac513cbb49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "364720107e0041bc8cc60148675a0ae9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fe0bf255eba43859f9d207646f1255c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49d624d6489f44dda136bf79960314fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3fe0bf255eba43859f9d207646f1255c",
      "placeholder": "​",
      "style": "IPY_MODEL_0b290cfa6bfc4cd4844200d2b088a528",
      "value": "100%"
     }
    },
    "5c7042457920424384aeb4656a447ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1990dda9bd24d6692447b4258809c98",
      "placeholder": "​",
      "style": "IPY_MODEL_6c630a6f97404188aaf853956b81d34c",
      "value": " 5/5 [00:21&lt;00:00,  4.35s/it]"
     }
    },
    "65082ee43a844f62ac8287b8faef5b66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c630a6f97404188aaf853956b81d34c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6edd5962dc354834993e41c0d54c94de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49d624d6489f44dda136bf79960314fc",
       "IPY_MODEL_b4fdcf9c7cab40319e09a91ff1a32e75",
       "IPY_MODEL_5c7042457920424384aeb4656a447ef5"
      ],
      "layout": "IPY_MODEL_364720107e0041bc8cc60148675a0ae9"
     }
    },
    "72a8540e9ce24b0f9c38a4a29c1377de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78cb849d96014a108015bd3f92f2b968": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a5d33c83dac4086a8cd16b226b59dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_159726388b174e5caa99596a7dba1757",
       "IPY_MODEL_dc7e4461995942a9855567ee63808372",
       "IPY_MODEL_93cf61378d324a7498665c969526ce92"
      ],
      "layout": "IPY_MODEL_9b97df51f0764ba09eebd9fb8cac9380"
     }
    },
    "81a7a92d5f6a4e15a332226aab686610": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83fdc33f980d41c4a600531fbeab2fc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93cf61378d324a7498665c969526ce92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4763079e73a4982a540c05cd82a00d2",
      "placeholder": "​",
      "style": "IPY_MODEL_78cb849d96014a108015bd3f92f2b968",
      "value": " 10/10 [03:41&lt;00:00, 21.82s/it]"
     }
    },
    "9b97df51f0764ba09eebd9fb8cac9380": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2d8ebaf188b481ba242ff07990b581d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4763079e73a4982a540c05cd82a00d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4fdcf9c7cab40319e09a91ff1a32e75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81a7a92d5f6a4e15a332226aab686610",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2feb36240afe434495d2bc45053d1103",
      "value": 5
     }
    },
    "c21c86b35c1c4b178d23f884add233bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c21d4335872e463590799841ec35b1eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72a8540e9ce24b0f9c38a4a29c1377de",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_303ef90204bd48e3a5423fac513cbb49",
      "value": 10
     }
    },
    "c9928da49da140ee9f7c647d9c4dd25b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cca2526aa9dc49eebd3e55e2f553e89b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1990dda9bd24d6692447b4258809c98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db1ebda23ce8424a857d3e73846fc0df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c21c86b35c1c4b178d23f884add233bb",
      "placeholder": "​",
      "style": "IPY_MODEL_cca2526aa9dc49eebd3e55e2f553e89b",
      "value": " 10/10 [00:40&lt;00:00,  3.96s/it]"
     }
    },
    "dc7e4461995942a9855567ee63808372": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc8fc8cbe5e949e6a279f2692cff54aa",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65082ee43a844f62ac8287b8faef5b66",
      "value": 10
     }
    },
    "dc8fc8cbe5e949e6a279f2692cff54aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e139d57637af473e9254cdbb5da91fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebfed28360ad4dffab39a9d23ec8bbfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2d8ebaf188b481ba242ff07990b581d",
      "placeholder": "​",
      "style": "IPY_MODEL_83fdc33f980d41c4a600531fbeab2fc0",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
