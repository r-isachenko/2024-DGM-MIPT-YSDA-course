{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRCQvjhe9FYV"
   },
   "source": [
    "# Homework4: Denoising score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ptW8_Cp-0W6"
   },
   "source": [
    "## Task 1: Theory (5pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbg3n5qXZRWW"
   },
   "source": [
    "### Problem 1: Conjugate functions / f-GAN (1pt)\n",
    "\n",
    "We have discussed the framework for f-divergence minimization at the Lecture 7. There we have got the variational inequality for f-divergence using Fenchel conjugate function:\n",
    "$$\n",
    "    D_f(\\pi || p) \\geq \\sup_{T \\in \\mathcal{T}} \\left[\\mathbb{E}_{\\pi}T(\\mathbf{x}) -  \\mathbb{E}_p f^*(T(\\mathbf{x})) \\right].\n",
    "$$\n",
    "Here\n",
    "$$\n",
    "\tf^*(t) = \\sup_{u \\in \\text{dom}_f} \\left( ut - f(u) \\right), \\quad f(u) = \\sup_{t \\in \\text{dom}_{f^*}} \\left( ut - f^*(t) \\right).\n",
    "$$\n",
    "\n",
    "In this task you have to derive standard GAN objective from the variational inequality.\n",
    "\n",
    "Let define function $f(u) = u \\log u - (u + 1) \\log (u + 1)$.\n",
    "\n",
    "- Find $\\text{dom}(f)$.\n",
    "- Show that $f^*(t) = - \\log (1 - e^t)$.\n",
    "- Use reparametrization $T(\\mathbf{x}) = \\log D(\\mathbf{x})$ to get the standard GAN objective (note that $D(\\mathbf{x}) \\in (0, 1)$, explain why this reparametrization is correct).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UIfd2ibZz-3"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC5tQ0odEmgM"
   },
   "source": [
    "### Problem 2: Frechet Inception Distance  (2pt)\n",
    "Let prove the theorem from the Lecture 7.\n",
    "Remember the Wasserstein metric:\n",
    "$$\n",
    "    W_s(\\pi, p) = \\inf_{\\gamma \\in \\Gamma(\\pi, p)} \\left(\\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim \\gamma} \\| \\mathbf{x} - \\mathbf{y} \\|^s\\right)^{1/s}\n",
    "$$\n",
    "\n",
    "Consider the case $\\mathbf{x} \\sim \\pi(\\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma_1^2 \\mathbf{I})$, $\\mathbf{y} \\sim p(\\mathbf{y}) = \\mathcal{N}(\\boldsymbol{\\mu}_2, \\sigma_2^2 \\mathbf{I})$.\n",
    "\n",
    "Let prove that in this case\n",
    "$$\n",
    "    W_2^2(\\pi, p) = \\inf_{\\gamma \\in \\Gamma(\\pi, p)} \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y}) \\sim \\gamma} \\| \\mathbf{x} - \\mathbf{y} \\|^2 = \\| \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2 \\|^2 + m \\cdot (\\sigma_1 - \\sigma_2)^2.\n",
    "$$\n",
    "Here $m$ is a dimensionality of the space ($\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^m$).\n",
    "\n",
    "**Hints:** (one of the possible solutions)\n",
    "1. Consider the case $\\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 = 0$.\n",
    "2. Use Cauchyâ€“Schwarz inequality to prove that the value given above is a minimal.\n",
    "2. Find the analytical mapping between $\\mathbf{x}$ and $\\mathbf{y}$ that gives this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-IpxlITHiF_"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n_qLVZe2Xq2"
   },
   "source": [
    "### Problem 3: Implicit score matching (2pt)\n",
    "\n",
    "We have discussed score matching task at Lecture 8. The objective of score matching is\n",
    "$$\n",
    "    \\frac{1}{2} \\mathbb{E}_{\\pi}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x}) - \\nabla_\\mathbf{x} \\log \\pi(\\mathbf{x}) \\bigr\\|^2_2 \\rightarrow \\min_{\\boldsymbol{\\theta}}.\n",
    "$$\n",
    "\n",
    "And we have already known one possible solution for this task. It is denoising score matching.\n",
    "\n",
    "Here our goal is to derive one more way to solve the initial score matching problem. It is called **implicit score matching**.\n",
    "\n",
    "Let consider 1-d case ($x \\in \\mathbb{R}$).\n",
    "Prove that\n",
    "$$\n",
    "\\frac{1}{2} \\mathbb{E}_{\\pi}\\bigl\\| s_{\\boldsymbol{\\theta}}(x) - \\nabla_x \\log \\pi(x) \\bigr\\|^2_2 = \\mathbb{E}_{\\pi}\\left[ \\frac{1}{2}s^2_{\\boldsymbol{\\theta}}(x) + \\nabla_{x} s_{\\boldsymbol{\\theta}}(x) \\right] + \\text{const}.\n",
    "$$\n",
    "\n",
    "- **Q:** Why is the expression at the right hand side better than the left one? **A:** It is better because we do not have the term with the unknown distribution $\\pi(x)$.\n",
    "\n",
    "- **Q:** Why do we not use this expression instead of denoising score matching? **A:** In this expression we have term $\\nabla_{x} s_{\\boldsymbol{\\theta}}(x) = \\nabla^2_{x} \\log p(x | \\boldsymbol{\\theta})$. And it is difficult to work with the second derivates.\n",
    "\n",
    "- **Q:** Why do we consider only 1-d case? **A:** It is very straightforward to generalize this formula to the multidimensional case, but the derivation contains much more formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "fJQYtKgA5ate",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "90a4a0b8-e86b-4edf-fbbf-7a329cb90dc6"
   },
   "outputs": [],
   "source": [
    "COMMIT_HASH = \"2004cdc1373b266959c0317939de90a9e3ca016c\"\n",
    "!if [ -d dgm_utils ]; then rm -Rf dgm_utils; fi\n",
    "!git clone https://github.com/r-isachenko/dgm_utils.git\n",
    "%cd dgm_utils\n",
    "!git checkout {COMMIT_HASH}\n",
    "!pip install ./\n",
    "%cd ./..\n",
    "!rm -Rf dgm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iXhgRngvaed4"
   },
   "outputs": [],
   "source": [
    "from dgm_utils import train_model\n",
    "from dgm_utils import show_samples, visualize_images, load_dataset, visualize_2d_data, visualize_2d_samples\n",
    "from dgm_utils import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVBow6mXafwx",
    "outputId": "1c2e39dd-3fe8-410a-f0cc-e10360260224"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print('GPU found :)')\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print('GPU not found :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehGykW1bnNbo"
   },
   "source": [
    "## Task 2: Denoising score matching for 2D data (3 pts)\n",
    "\n",
    "In this task you will implement the denoising score matching model to the 2D moons dataset.\n",
    "\n",
    "Let's take a look at dataset samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P2zCpepU3Bvw"
   },
   "outputs": [],
   "source": [
    "def generate_moons_data(count: int) -> tuple:\n",
    "    data, labels = make_moons(n_samples=count, noise=0.1)\n",
    "    data = data.astype(\"float32\")\n",
    "    split = int(0.8 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    train_labels, test_labels = labels[:split], labels[split:]\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "bJUHA3sy3Etb",
    "outputId": "a7fbcdd9-5a35-4850-b032-49017aa046fa"
   },
   "outputs": [],
   "source": [
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "visualize_2d_data(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07H5Put_nbTI"
   },
   "source": [
    "Let recall the theory of denoising score matching.\n",
    "\n",
    "The idea is the following. We define the score function\n",
    "$$\n",
    "    \\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}| \\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "Then we minimize the Fisher divergence to obtain the score function:\n",
    "$$\n",
    "    D_F(\\pi, p) = \\frac{1}{2}\\mathbb{E}_{\\pi}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x}) - \\nabla_{\\mathbf{x}} \\log \\pi(\\mathbf{x}) \\bigr\\|^2_2 \\rightarrow \\min_{\\boldsymbol{\\theta}}\n",
    "$$.\n",
    "\n",
    "If we have the score function, we use the Langevin dynamics to sample from our model:\n",
    "$$\n",
    "    \\mathbf{x}_{l + 1} = \\mathbf{x}_l + \\frac{\\eta}{2} \\cdot \\nabla_{\\mathbf{x}_l} \\log p(\\mathbf{x}_l | \\boldsymbol{\\theta}) + \\sqrt{\\eta} \\cdot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "But Fisher divergence is intractable and we use the noising procedure to get noised samples $\\mathbf{x}_{\\sigma} = \\mathbf{x} + \\sigma \\cdot \\boldsymbol{\\epsilon}$.\n",
    "\n",
    "Minimizing the Fisher divergence for the noisy samples is equivalent to the following objective:\n",
    "$$\n",
    "\\mathbb{E}_{q(\\mathbf{x}_{\\sigma})}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}, \\sigma}(\\mathbf{x}_{\\sigma}) - \\nabla_{\\mathbf{x}_{\\sigma}} \\log q(\\mathbf{x}_{\\sigma}) \\bigr\\|^2_2 = \\mathbb{E}_{\\pi(\\mathbf{x})} \\mathbb{E}_{q(\\mathbf{x}_{\\sigma} | \\mathbf{x})}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}, \\sigma}(\\mathbf{x}_{\\sigma}) - \\nabla_{\\mathbf{x}_{\\sigma}} \\log q(\\mathbf{x}_{\\sigma} | \\mathbf{x}) \\bigr\\|^2_2 + \\text{const}(\\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "Here\n",
    "$$\n",
    "    \\log q(\\mathbf{x}_{\\sigma} | \\mathbf{x}) = - \\frac{\\mathbf{x}_{\\sigma} - \\mathbf{x}}{\\sigma^2} = - \\frac{\\boldsymbol{\\epsilon}}{\\sigma}.\n",
    "$$\n",
    "\n",
    "Therefore, the objective of the denoising score matching is\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\pi(\\mathbf{x})} \\mathbb{E}_{q(\\mathbf{x}_{\\sigma} | \\mathbf{x})}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}, \\sigma}(\\mathbf{x}_{\\sigma}) + \\frac{\\boldsymbol{\\epsilon}}{\\sigma} \\bigr\\|^2_2 \\rightarrow \\min_{\\boldsymbol{\\theta}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0ikuwxWM3Nba"
   },
   "outputs": [],
   "source": [
    "class DenoisingScoreMatcher(BaseModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            score_model: nn.Module,\n",
    "            input_shape: Tuple[int],\n",
    "            sigma: float\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.score_model = score_model\n",
    "        self.input_shape = input_shape\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # ====\n",
    "        # your code\n",
    "        # sample gaussian noise\n",
    "        # perturb samples using the noise and sigma\n",
    "\n",
    "        # =====\n",
    "\n",
    "        # calculate the score model\n",
    "        s = self.score_model(noisy_x)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # compute the loss\n",
    "        # it is mse between score function and gradient of the normal distribution\n",
    "\n",
    "        # =====\n",
    "        return loss\n",
    "\n",
    "    def loss(self, x: torch.Tensor):\n",
    "        return {\"total_loss\": self(x).mean(dim=0).sum()}\n",
    "\n",
    "    def langevin_dynamics(self, x: torch.Tensor, num_steps: int, eta: float):\n",
    "        # =====\n",
    "        # your code\n",
    "        # apply Langevin dynamics in for-cycle to the starting point x\n",
    "        \n",
    "        # =====\n",
    "        return x\n",
    "\n",
    "    def sample(self, num_samples: int = 64, num_steps: int=100, eta: float = 0.01):\n",
    "        with torch.no_grad():\n",
    "            # we sample x_0 from U[-1, 1]\n",
    "            x0 = 2. * torch.rand_like(torch.empty(num_samples, *self.input_shape)) - 1.\n",
    "            x0 = x0.to(self.device)\n",
    "\n",
    "            # run langevine dynamics\n",
    "            x = self.langevin_dynamics(x0, num_steps=num_steps, eta=eta)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_denoiser_score_matcher():\n",
    "    matcher = DenoisingScoreMatcher(\n",
    "        score_model=nn.Linear(2, 2),\n",
    "        input_shape=(2,),\n",
    "        sigma=0.1\n",
    "    )\n",
    "    x = torch.rand(16, 2)\n",
    "    assert x.size() == matcher(x).size()\n",
    "    loss = matcher.loss(x)[\"total_loss\"]\n",
    "    assert len(loss.size()) == 0\n",
    "    assert list(matcher.sample(4).size()) == [4, 2]\n",
    "\n",
    "\n",
    "test_denoiser_score_matcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaIv3M6crme-"
   },
   "source": [
    "That's all!\n",
    "\n",
    "And now we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 915
    },
    "id": "ft98D6FT7FbY",
    "outputId": "36b02981-0d04-4590-bb2c-d243795ce879"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "HIDDEN_SIZE = \n",
    "SIGMA = \n",
    "# ====\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# define sequential model\n",
    "# it is enough to use the sequence of Linear layers with activations\n",
    "\n",
    "# ====\n",
    "\n",
    "matcher = DenoisingScoreMatcher(\n",
    "    score_model=score_model, input_shape=(2,), sigma=SIGMA\n",
    ")\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# choose any optimizer/scheduler as you want\n",
    "optimizer = torch.optim.Adam(matcher.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "# ====\n",
    "\n",
    "train_model(\n",
    "    matcher,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    n_samples=1024,\n",
    "    visualize_samples=True,\n",
    "    logscale_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLUKPrror7GJ"
   },
   "source": [
    "Let sample from our model. Experiment with number of steps and $\\eta$ for Langevin dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "dQk73I-G_CVa",
    "outputId": "288009ca-fb1f-435a-d801-dace05be1269"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "NUM_STEPS = \n",
    "ETA = \n",
    "# ====\n",
    "\n",
    "samples = matcher.sample(num_samples=5000, num_steps=NUM_STEPS, eta=ETA).cpu()\n",
    "\n",
    "visualize_2d_samples(samples, title=\"Samples\", s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovZUhvWCsANm"
   },
   "source": [
    "## Task 3: Noise Conditioned Score Network for MNIST (5 pts)\n",
    "\n",
    "Now we try to extend our model to the NCSN. It means that we have to add multiple noise scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "id": "QZV79VwSEAXZ",
    "outputId": "9333ca5a-8e8f-465c-ca76-867bf86e6b3c"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=False)\n",
    "visualize_images(train_data, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izZsvHvIsJHU"
   },
   "source": [
    "Here we will use the resnet-like architecture. But we encourage you to experiment with it.\n",
    "\n",
    "The important thing here is the conditioning of the score model to noise. It means that the noise scale $\\sigma$ have to be the input of the model. We will use embedding layer to make this conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "U99Orvw4H_4E"
   },
   "outputs": [],
   "source": [
    "class ConditionedResnetBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_embeddings: int) -> None:\n",
    "        super().__init__()\n",
    "        # you could experiment with this architecture\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dim, dim, kernel_size=1),\n",
    "        )\n",
    "        self.dim = dim\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        time_embed = self.embedding(y).view(-1, self.dim, 1, 1)\n",
    "        return x + self.block(x + time_embed)\n",
    "\n",
    "\n",
    "class ConditionedSimpleResnet(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, n_filters: int, n_blocks: int, num_embeddings: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # you could experiment with this architecture\n",
    "        self.first_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n_filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layers = nn.Sequential(*[ConditionedResnetBlock(n_filters, num_embeddings) for _ in range(n_blocks)])\n",
    "        self.last_block = nn.Sequential(\n",
    "            nn.ReLU(), nn.Conv2d(n_filters, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.n_filters = n_filters\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.first_block(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, y)\n",
    "        x = self.last_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_conditioned_resnet():\n",
    "    model = ConditionedSimpleResnet(in_channels=1, out_channels=1, n_filters=16, n_blocks=1, num_embeddings=2)\n",
    "    x = torch.rand((1, 1, 28, 28))\n",
    "    y = torch.zeros(size=(1,), dtype=torch.long)\n",
    "    out1 = model(x, y)\n",
    "    y = torch.ones(size=(1,), dtype=torch.long)\n",
    "    out2 = model(x, y)\n",
    "    assert not np.allclose(out1.detach().numpy(), out2.detach().numpy())\n",
    "\n",
    "\n",
    "test_conditioned_resnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define the main model.\n",
    "\n",
    "We will use the sequence of the noise levels: $\\sigma_1 < \\sigma_2 < \\dots < \\sigma_T$. In this task it will be the geometric progression.\n",
    "And we will perturb the original data with the different noise levels to obtain \n",
    "$$\n",
    "\\mathbf{x}_t = \\mathbf{x} + \\sigma_t \\cdot \\boldsymbol{\\epsilon}, \\quad \\mathbf{x}_t \\sim q(\\mathbf{x}_t). \n",
    "$$\n",
    "\n",
    "Our training objective:\n",
    "$$\n",
    "    \\sum_{t=1}^T \\frac{\\sigma_t^2}{\\sigma_T^2} \\mathbb{E}_{\\pi(\\mathbf{x})} \\mathbb{E}_{q(\\mathbf{x}_t | \\mathbf{x})}\\bigl\\| \\mathbf{s}_{\\boldsymbol{\\theta}, \\sigma_t}(\\mathbf{x}_t) - \\nabla_{\\mathbf{x}_{\\sigma}} \\log q(\\mathbf{x}_t | \\mathbf{x}) \\bigr\\|^2_2 \\rightarrow \\min_{\\boldsymbol{\\theta}}\n",
    "$$\n",
    "But instead of doing the honest summation we will sample one timestamp for each sample.\n",
    "\n",
    "We will use annealed Langevin dynamics to sample from our model:\n",
    "1. Sample $\\mathbf{x}_0 \\sim \\mathcal{N}(0, \\sigma_T^2 \\cdot \\mathbf{I}) \\approx q(\\mathbf{x}_T)$.\n",
    "2. Apply $L$ steps of Langevin dynamic\n",
    "$$\n",
    "    \\mathbf{x}_l = \\mathbf{x}_{l-1} + \\frac{\\eta_t}{2} \\cdot \\mathbf{s}_{\\boldsymbol{\\theta}, \\sigma_t}(\\mathbf{x}_{l - 1}) + \\sqrt{\\eta_t} \\cdot \\boldsymbol{\\epsilon}_l.\n",
    "$$\n",
    "3. Update $\\mathbf{x}_0 := \\mathbf{x}_L$ and choose the next $\\sigma_t$.\n",
    "4. Repeat it for all sigmas.\n",
    "\n",
    "**Note:** use the following formula for $\\eta_t = \\epsilon \\cdot \\frac{\\sigma_t^2}{\\sigma_T^2}$ ($\\epsilon$ is a small number that is a hyperparameter of the sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MMSV8FqMV1h8"
   },
   "outputs": [],
   "source": [
    "class NoiseConditionedScoreNetwork(BaseModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            score_model: nn.Module,\n",
    "            input_shape: Tuple[int],\n",
    "            sigmas: List[float]\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.score_model = score_model\n",
    "        self.input_shape = input_shape\n",
    "        self.sigmas = torch.FloatTensor(sorted(sigmas, reverse=True))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        self.sigmas = self.sigmas.to(self.device)\n",
    "        batch_size = x.shape[0]\n",
    "        # ====\n",
    "        # your code\n",
    "        # sample gaussian noise\n",
    "        # sample timestamps for each datapoint in the batch\n",
    "        # choose sigmas for these datapoints\n",
    "        # add noises to the x samples\n",
    "        \n",
    "        # =====\n",
    "\n",
    "        # calculate the score model\n",
    "        s = self.score_model(noisy_x, which_sigmas)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # compute the loss\n",
    "        # it is mse between score function and gradient of the normal distribution (do not forget the coefficient before the mse)\n",
    "        \n",
    "        # =====\n",
    "        return loss\n",
    "\n",
    "    def loss(self, x: torch.Tensor):\n",
    "        return {\"total_loss\": self(x).mean(dim=0).sum()}\n",
    "\n",
    "    def annealed_langevin_dynamics(self, x: torch.Tensor, num_steps: int, eps: float):\n",
    "        # =====\n",
    "        # your code\n",
    "        # here we will have 2 cycles: one for sigmas, one for Langevin sampling\n",
    "        # start with the largest sigma, apply Langevin dynamic for it and move to the next sigma\n",
    "        \n",
    "        # =====\n",
    "        return x\n",
    "    @torch.no_grad()\n",
    "    def sample(self, num_samples: int = 64, num_steps: int=100, eps: float = 0.1):\n",
    "        # we sample x_0 from U[-1, 1]\n",
    "        x0 = 2. * torch.rand_like(torch.empty(num_samples, *self.input_shape)) - 1.\n",
    "        x0 = x0.to(self.device)\n",
    "\n",
    "        # run langevine dynamics\n",
    "        x = self.annealed_langevin_dynamics(x0, num_steps=num_steps, eps=eps)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_ncsn():\n",
    "    class DummyConditionedMLP(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer = nn.Linear(2, 2)\n",
    "\n",
    "        def forward(self, x: torch.Tensor, y: torch.Tensor):\n",
    "            y = y.view(-1, 1)\n",
    "            return self.layer(x) + y\n",
    "\n",
    "\n",
    "    ncsn = NoiseConditionedScoreNetwork(\n",
    "        score_model=DummyConditionedMLP(),\n",
    "        input_shape=(2,),\n",
    "        sigmas=[0.1]\n",
    "    )\n",
    "    x = torch.rand(16, 2)\n",
    "    assert x.size() == ncsn(x).size()\n",
    "    loss = ncsn.loss(x)[\"total_loss\"]\n",
    "    assert len(loss.size()) == 0\n",
    "    assert list(ncsn.sample(4).size()) == [4, 2]\n",
    "\n",
    "\n",
    "test_ncsn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "OlQLodUHGoYq",
    "outputId": "01005210-060a-49f6-dc8e-c5275f02cfc3"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE =   \n",
    "LR =\n",
    "EPOCHS = \n",
    "N_FILTERS =\n",
    "N_BLOCKS =\n",
    "\n",
    "L = \n",
    "SIGMA_MIN = \n",
    "SIGMA_MAX = \n",
    "q = \n",
    "SIGMAS = \n",
    "# ====\n",
    "\n",
    "train_loader = data.DataLoader(2 * train_data - 1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(2 * test_data - 1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "score_model = ConditionedSimpleResnet(\n",
    "     in_channels=1, out_channels=1, n_filters=N_FILTERS, n_blocks=N_BLOCKS, num_embeddings=len(SIGMAS)\n",
    ")\n",
    "\n",
    "ncsn = NoiseConditionedScoreNetwork(\n",
    "    score_model=score_model, input_shape=(1, 28, 28), sigmas=SIGMAS,\n",
    ")\n",
    "\n",
    "# ====\n",
    "# your code\n",
    "# choose any optimizer/scheduler as you want\n",
    "optimizer = torch.optim.Adam(ncsn.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "# ====\n",
    "\n",
    "# train\n",
    "train_model(\n",
    "    ncsn,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    n_samples=4,\n",
    "    visualize_samples=True,\n",
    "    logscale_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "wjZS5HMpMb5t",
    "outputId": "b48864a8-985e-44dc-ab3b-6d23c2139dfd"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "NUM_STEPS = \n",
    "EPS = \n",
    "# ====\n",
    "\n",
    "samples = ncsn.sample(100, num_steps=NUM_STEPS, eps=EPS).cpu()\n",
    "show_samples(samples, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_-FwyrzNvgP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0057c56e43504274858faf340424348c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "144b17bde62042ac86f4c3507240ac1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1f880ad04f534e08aad0a006a33ed646": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fca1a010dc84a2aadfc6b4e5f15d1f6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d013d0fa6ee74b4ead7dc4f66d55f77c",
      "value": "100%"
     }
    },
    "25a3dc242d10469abc5a211dbc0746ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb1bf34d7e344a5f9f459c0905548652",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_144b17bde62042ac86f4c3507240ac1e",
      "value": 7
     }
    },
    "2d52084a512f4055ad01a9afbfaffff5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f880ad04f534e08aad0a006a33ed646",
       "IPY_MODEL_25a3dc242d10469abc5a211dbc0746ca",
       "IPY_MODEL_a9b2b099d36149dda1a92bb1eece775b"
      ],
      "layout": "IPY_MODEL_d3b97e8914794f98bf81d84a2aadfe52"
     }
    },
    "518e7454eaa14f10ada3781a2401b622": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "639c32ab53164d7f9df57757b6e5c9c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "77b9f77796ab4344a23847d02e2087e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d0563bf68604b8b9614d4fe0a8f992b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_950787f7d6934287a86a0a38c57ae29d",
       "IPY_MODEL_89eb7c59a8e84fefa0db7f440f6848bb",
       "IPY_MODEL_8afb4f28944145c3b90bacb6cb3b6327"
      ],
      "layout": "IPY_MODEL_77b9f77796ab4344a23847d02e2087e0"
     }
    },
    "7d18d94ae32941f7adfab36d7b49f61e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89eb7c59a8e84fefa0db7f440f6848bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0057c56e43504274858faf340424348c",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_639c32ab53164d7f9df57757b6e5c9c2",
      "value": 3
     }
    },
    "8afb4f28944145c3b90bacb6cb3b6327": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a25e239d5cfd4c04a0569ee909df63e5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bf5f3024f475431ca24c96f147255d75",
      "value": "â€‡3/20â€‡[02:34&lt;11:23,â€‡40.20s/it]"
     }
    },
    "8fca1a010dc84a2aadfc6b4e5f15d1f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "950787f7d6934287a86a0a38c57ae29d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f29ce587aa53499c9c9e9c589be8f320",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_518e7454eaa14f10ada3781a2401b622",
      "value": "â€‡15%"
     }
    },
    "a25e239d5cfd4c04a0569ee909df63e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9b2b099d36149dda1a92bb1eece775b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abc7b4bf2975473e8de753c032e6e212",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7d18d94ae32941f7adfab36d7b49f61e",
      "value": "â€‡7/7â€‡[03:02&lt;00:00,â€‡25.82s/it]"
     }
    },
    "abc7b4bf2975473e8de753c032e6e212": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf5f3024f475431ca24c96f147255d75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d013d0fa6ee74b4ead7dc4f66d55f77c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3b97e8914794f98bf81d84a2aadfe52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f29ce587aa53499c9c9e9c589be8f320": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb1bf34d7e344a5f9f459c0905548652": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
