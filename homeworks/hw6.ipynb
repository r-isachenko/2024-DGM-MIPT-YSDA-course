{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZXJoDiD_x-N"
   },
   "source": [
    "# Homework6: Flow models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxF8ewFXn1HO",
    "tags": []
   },
   "source": [
    "## Task 1: Theory (5pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFn6d3vkuZIl"
   },
   "source": [
    "### Problem 1: KFP theorem (1pt)\n",
    "\n",
    "We have faced with 2 different formulations of Kolmogorov-Fokker-Planck theorem.\n",
    "\n",
    "1) continuity equation in continuous-in-time NF:\n",
    "$$\n",
    "\\frac{d \\log p(\\mathbf{x}(t), t)}{d t} = - \\text{tr} \\left( \\frac{\\partial f(\\mathbf{x}, t)}{\\partial \\mathbf{x}} \\right);\n",
    "$$\n",
    "\n",
    "2) the general form of the KFP equation in SDEs:\n",
    "$$\n",
    "\\frac{\\partial p(\\mathbf{x}, t)}{\\partial t} = - \\text{div}\\left(\\mathbf{f}(\\mathbf{x}, t) p(\\mathbf{x}, t)\\right) + \\frac{1}{2} g^2(t) \\Delta p(\\mathbf{x}, t).\n",
    "$$\n",
    "\n",
    "In this task your goal is to prove that the first formulation is a special case of the more general second formulation.\n",
    "\n",
    "**Note:** The derivation in the first formulation is total derivative (not partial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbxPyzwcuinj"
   },
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: DDPM as SDE discretization (2pt)\n",
    "\n",
    "We have proved that DDPM is a discretization of the SDE\n",
    "$$\n",
    "\td \\mathbf{x} = - \\frac{1}{2} \\beta(t) \\mathbf{x}(t) dt + \\sqrt{\\beta(t)} \\cdot d \\mathbf{w}.\n",
    "$$\n",
    "Here $\\mathbf{f}(\\mathbf{x}, t) = - \\frac{1}{2} \\beta(t) \\mathbf{x}(t)$, $g(t) = \\sqrt{\\beta(t)}$.\n",
    "\n",
    "Recall reverse SDE\n",
    "$$\n",
    "    d\\mathbf{x} = \\left(\\mathbf{f}(\\mathbf{x}, t) - g^2(t) \\frac{\\partial \\log p_t(\\mathbf{x})}{\\partial \\mathbf{x}}\\right) dt + g(t) d \\mathbf{w}.\n",
    "$$\n",
    "\n",
    "The reverse SDE of the DDPM model will be\n",
    "$$\n",
    "    d\\mathbf{x}(t) = -\\beta(t)\\left[\\frac{x(t)}{2} + \\nabla_{\\mathbf{x}}\\log p_t(\\mathbf{x}(t))\\right]dt + \\sqrt{\\beta(t)}d\\mathbf{w}.\n",
    "$$\n",
    "\n",
    "The DDPM uses the following form of ancestral sampling\n",
    "$$\n",
    "\\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{1 - \\beta_t}} \\cdot \\mathbf{x}_t + \\frac{\\beta_t}{\\sqrt{1 - \\beta_t}} \\cdot \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t | \\boldsymbol{\\theta}) +  \\sqrt{\\beta_t} \\cdot \\boldsymbol{\\epsilon}.\n",
    "$$\n",
    "(Here we assumed that $p(\\mathbf{x}_{t - 1} | \\mathbf{x}_t, \\boldsymbol{\\theta}) = \\mathcal{N} \\bigl(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}, t}(\\mathbf{x}_t), \\beta_t \\cdot \\mathbf{I}\\bigr)$).\n",
    "\n",
    "Here is your task to validate that DDPM iterative update scheme is actually discretization of SDE by letting $t \\in \\{0,\\ldots,\\frac{N-1}{N}\\}$, $\\Delta t = 1/N$, $\\mathbf{x}(t-\\Delta t) = \\mathbf{x}_{s-s}$, $\\mathbf{x}(t) = \\mathbf{x}_s$, and $\\beta(t)\\Delta t = \\beta_s$, s.e.:\n",
    "\n",
    "In this task your goal is to show that the ancestral sampling is a discretization of the DDPM reverse SDE.\n",
    "\n",
    "**Hints**:\n",
    "1. use $dt < 0$;\n",
    "2. $\\beta_t = - \\beta(t) dt$;\n",
    "3. $d\\mathbf{w} = \\boldsymbol{\\epsilon} \\cdot \\sqrt{-dt}$;\n",
    "4. drop the terms with the order of $o(dt)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Flow matching distribution (2pt)\n",
    "\n",
    "Let consider flow matching model between two same distributions:\n",
    "$$\n",
    "    p_0(x) = \\mathcal{N}(0, \\sigma^2) \\quad p_1(x) = \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Your goal is to find the analytical expression for distribution $p_t(x_t)$.\n",
    "\n",
    "**Note:** you have to get nonlinear expression for variance, try to understand this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "your solution\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41370,
     "status": "ok",
     "timestamp": 1700068971438,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "nYZ__zsi3McN",
    "outputId": "1153bb7c-f9cd-4a0f-83bd-e2827f989f53",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COMMIT_HASH = \"11668881e2da2ea7938417bdabda0397660508c8\"\n",
    "!if [ -d dgm_utils ]; then rm -Rf dgm_utils; fi\n",
    "!git clone https://github.com/r-isachenko/dgm_utils.git\n",
    "%cd dgm_utils\n",
    "!git checkout {COMMIT_HASH}\n",
    "!pip install ./\n",
    "!pip install torchdiffeq\n",
    "%cd ./..\n",
    "!rm -Rf dgm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mC57wAo5Yag"
   },
   "outputs": [],
   "source": [
    "from dgm_utils import load_dataset, BaseModel, train_model\n",
    "from dgm_utils import visualize_images, visualize_2d_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1700068977911,
     "user": {
      "displayName": "Роман Исаченко",
      "userId": "08996523319375397632"
     },
     "user_tz": -180
    },
    "id": "grmP96FjfQZg",
    "outputId": "19c07923-08ff-45a4-e607-34e3c5bc325e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.distributions.normal import Normal\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print('GPU found :)') \n",
    "else: \n",
    "    DEVICE = \"cpu\"\n",
    "    print('GPU not found :(')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Continuous-time Normalizing Flows (4 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you have to implement Continuous-time Normalizing Flow and apply it to 2D dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 5_000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_dataset('moons', size=COUNT, with_targets=True)\n",
    "visualize_2d_data(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit continuous normalizing flows (CNFs).\n",
    "\n",
    "In CNFs, a central task is efficiently computing derivatives, particularly the trace of the Jacobian of the dynamics function $f(\\mathbf{x}(t), t)$. As we saw in Lecture 11, the change in log-probability over time is given by:\n",
    "\n",
    "$$\n",
    "\\frac{d \\log p(\\mathbf{x}(t))}{dt} = -\\text{Tr}\\left( \\frac{\\partial f(\\mathbf{x}(t), t)}{\\partial \\mathbf{x}(t)} \\right).\n",
    "$$\n",
    "\n",
    "In high-dimensional spaces computing the exact trace of the Jacobian $\\frac{\\partial f}{\\partial \\mathbf{x}}$ can be computationally expensive. To overcome this challenge, could be used **Hutchinson's Trace Estimator**, which provides an **efficient** and **unbiased** estimate:\n",
    "\n",
    "$$\n",
    "\\text{Tr}\\left( \\frac{\\partial f}{\\partial \\mathbf{x}} \\right) = \\mathbb{E}_{\\mathbf{\\epsilon} \\sim p(\\mathbf{\\epsilon})} \\left[ \\mathbf{\\epsilon}^\\top \\frac{\\partial f}{\\partial \\mathbf{x}} \\mathbf{\\epsilon} \\right],\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\epsilon}$ is a random vector sampled from a standard normal distribution $\\mathcal{N}(0, \\mathbf{I})$.\n",
    "\n",
    "**Note:** In practice, we approximate this expectation using a single sample of $\\mathbf{\\epsilon}$ to efficiently estimate the trace.\n",
    "\n",
    "However, since the data we consider in this task is only two-dimensional, we can easily compute the entire Jacobian. For this task we will use the same conditional model as in previous homeworks to parametrize $\\frac{d \\mathbf{x}(t)}{d t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, lets define time embedding layer, that works with values in range $[0, 1]$.\n",
    "\n",
    "**Note:** we can't use here `nn.Embedding`, because it takes only integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.register_buffer('freqs', torch.arange(1, dim // 2 + 1) * torch.pi)\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        emb = self.freqs * t.unsqueeze(-1)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbtFK3NF2F08"
   },
   "outputs": [],
   "source": [
    "class ConditionalMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.x_proj = nn.Linear(input_dim, self.hidden_dim)\n",
    "        self.t_proj = TimeEmbedding(self.hidden_dim)\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "        x = self.x_proj(x)\n",
    "        t = self.t_proj(t)\n",
    "        x = x + t\n",
    "        x = F.tanh(x)\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "def test_conditional_mlp():\n",
    "    SHAPE = [2, 20]\n",
    "    x = torch.ones(SHAPE)\n",
    "    t = torch.ones((2,)).long() * 5\n",
    "    model = ConditionalMLP(input_dim=20)\n",
    "    output = model(x, t)\n",
    "    assert list(output.shape) == SHAPE\n",
    "\n",
    "\n",
    "test_conditional_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNFModel(BaseModel):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.model = ConditionalMLP(input_dim, hidden_dim)\n",
    "        self.prior = Normal(torch.tensor(0.0), torch.tensor(1.0))\n",
    "    \n",
    "    def odefunc(self, t: torch.Tensor, states: Tuple[torch.Tensor]) -> Tuple[torch.Tensor]:\n",
    "        z, _ = states\n",
    "        with torch.set_grad_enabled(True):\n",
    "            z.requires_grad_(True)\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) apply model to get first order derivatives\n",
    "            # 2) get second order derivative using torch.autograd\n",
    "            # Do not forget to use epsilon\n",
    "            \n",
    "            # ====\n",
    "        return dz_dt, -trace\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, reverse: bool = False) -> Tuple[torch.Tensor]:\n",
    "        x = x.to(self.device)\n",
    "        dz_dt = torch.zeros([x.shape[0], 1], device=self.device)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # use odeint_adjoint to simulate self.odefunc\n",
    "        # use reverse to simulate from 1 to 0 timesteps\n",
    "        \n",
    "        # ====\n",
    "        \n",
    "        return z, dz_dt\n",
    "    \n",
    "    def loss(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        z, dz_dt = self(x)\n",
    "        # ====\n",
    "        # your code\n",
    "        # use CoV to get loglikelihood of p(x)\n",
    "        \n",
    "        # ====\n",
    "        return {'total_loss': loss}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        # read carefully the sampling process\n",
    "        z = self.prior.sample([n, self.input_dim]).to(self.device)\n",
    "        x, _ = self(z, reverse=True)\n",
    "        return x.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train the model, it takes some time :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "LR = \n",
    "EPOCHS = \n",
    "HIDDEN_DIM = \n",
    "# ====\n",
    "\n",
    "model = CNFModel(input_dim=2, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# try your own optimizer/scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    n_samples=1024,\n",
    "    visualize_samples=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Flow matching on MNIST (5 pt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, your task to train Flow matching model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(\"mnist\", flatten=False, binarize=True)\n",
    "visualize_images(train_data, \"MNIST samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10tMXiU8P_6v"
   },
   "source": [
    "The model is written for you. We will use conditioned ResNet architecture. But you could change it if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNyTMSl42F1G"
   },
   "outputs": [],
   "source": [
    "class ConditionedResnetBlock(nn.Module):\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # you could experiment with this architecture\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(dim, dim, kernel_size=1),\n",
    "        )\n",
    "        self.dim = dim\n",
    "        self.embedding = TimeEmbedding(dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        time_embed = self.embedding(t).view(-1, self.dim, 1, 1)\n",
    "        return x + self.block(x + time_embed)\n",
    "\n",
    "\n",
    "class ConditionedSimpleResnet(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, n_filters: int, n_blocks: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # you could experiment with this architecture\n",
    "        self.first_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n_filters, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layers = nn.Sequential(*[ConditionedResnetBlock(n_filters) for _ in range(n_blocks)])\n",
    "        self.last_block = nn.Sequential(\n",
    "            nn.ReLU(), nn.Conv2d(n_filters, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.n_filters = n_filters\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.first_block(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, t)\n",
    "        x = self.last_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_conditioned_resnet():\n",
    "    model = ConditionedSimpleResnet(in_channels=1, out_channels=1, n_filters=16, n_blocks=1)\n",
    "    x = torch.rand((1, 1, 28, 28))\n",
    "    t = torch.zeros(size=(1,), dtype=torch.long)\n",
    "    out1 = model(x, t)\n",
    "    t = torch.ones(size=(1,), dtype=torch.long)\n",
    "    out2 = model(x, t)\n",
    "    assert not np.allclose(out1.detach().numpy(), out2.detach().numpy())\n",
    "\n",
    "\n",
    "test_conditioned_resnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conditional flow matching, our objective is to learn a vector field $ f_\\theta(\\mathbf{x}, t) $, parameterized by a neural network, that aligns with a known target vector field $f(\\mathbf{x}, \\mathbf{x}_1, t)$ at each point along a path connecting the data distribution and a base distribution. So, the training objective is defined as:\n",
    "\n",
    "$$\n",
    "\\min_\\theta\\, \\mathbb{E}_{t \\sim U[0, 1]}\\, \\mathbb{E}_{\\mathbf{x}_1 \\sim p(\\mathbf{x}_1)} \\mathbb{E}_{\\mathbf{x} \\sim p_t(\\mathbf{x} | \\mathbf{x}_1)} \\left[ \\left\\| f(\\mathbf{x}, \\mathbf{x}_1, t) - f_\\theta(\\mathbf{x}, t) \\right\\|^2 \\right],\n",
    "$$\n",
    "\n",
    "In this task, we consider the **optimal transport conditional vector field**, defined by:\n",
    "$$\n",
    "f(\\mathbf{x}, \\mathbf{x}_1, t) = \\frac{d\\mathbf{x}}{dt} = \\frac{\\mathbf{x}_1 - (1 - \\sigma_{\\text{min}})\\mathbf{x}}{1 - (1 - \\sigma_{\\text{min}})t},\n",
    "$$\n",
    "which means that $\\mathbf{x}$ iterpolates linearly by making data more noisy:\n",
    "$$\n",
    "\\mathbf{x}_t = t \\mathbf{x}_1 + (1 - (1 - \\sigma_{\\text{min}})t) \\mathbf{x}_0.\n",
    "$$\n",
    "\n",
    "Now, let's define the architecture of the Flow Matching model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingModel(BaseModel):\n",
    "    def __init__(self, in_channels: int, out_channels: int, n_filters: int, n_blocks: int):\n",
    "        super().__init__()\n",
    "        self.model = ConditionedSimpleResnet(\n",
    "            in_channels, out_channels, n_filters, n_blocks\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        return self.model(x, t)\n",
    "\n",
    "    def loss(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        sigma_min = 1e-4\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) samle time uniformly from 0 to 1\n",
    "        # 2) calculate noised data and optimal flow\n",
    "        # 3) predict flow using model\n",
    "        # 4) calculate loss\n",
    "        \n",
    "        # ====\n",
    "        return {'total_loss': loss}\n",
    "\n",
    "    def odefunc(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self(x, torch.full(x.shape[:1], t, device=self.device))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n: int) -> np.ndarray:\n",
    "        # read carefully the sampling process\n",
    "        z = torch.randn(n, 1, 28, 28, device=self.device)  # Start with noise\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # use odeint to sample from model\n",
    "        # here we don't need to use adjoint because we use odeint only for sampling!\n",
    "        \n",
    "        # ====\n",
    "        samples = states[1]\n",
    "        return samples.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "BATCH_SIZE = \n",
    "LR = \n",
    "EPOCHS = \n",
    "N_FILTERS = \n",
    "N_BLOCKS = \n",
    "# ====\n",
    "\n",
    "model = FlowMatchingModel(\n",
    "    in_channels=1, \n",
    "    out_channels=1, \n",
    "    n_filters=N_FILTERS, \n",
    "    n_blocks=N_BLOCKS\n",
    ")\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "# choose any optimizer/scheduler as you want\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    n_samples=16,\n",
    "    visualize_samples=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
